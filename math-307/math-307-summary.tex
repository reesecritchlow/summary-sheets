\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper, portrait, margin=1in]{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\setlength\parindent{0pt}
\usepackage{enumerate}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{fancyhdr}
\usepackage{tcolorbox}
\hyphenchar\font=-1

\newcommand{\header}[1]{\begin{large}\noindent #1\end{large}\\\rule{\textwidth}{0.5pt}}
\newcommand{\gap}{\medskip\\}
\newcommand{\centertext}[1]{\begin{center}#1\end{center}}
\newcommand{\bfrac}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\formula}[3]{\begin{center} \begin{tcolorbox}[title = #2] $$#3$$\end{tcolorbox}\end{center}}
\newcommand{\where}{\hspace{0.5cm} \textrm{where} \hspace{0.5cm}}
\newcommand{\hgap}{\hspace{0.5cm}}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sheader}[1]{\underline{#1:}}
\newcommand{\doubleformula}[4]{\begin{center} \begin{tcolorbox}[title = #2] $$#3$$\\$$#4$$\end{tcolorbox}\end{center}}
\newcommand{\curly}[1]{\left\{#1\right\}}
\newcommand{\proj}[2]{{}\textrm{proj}_{#1}\left(#2\right)}

\newcommand{\tripleformula}[5]{\begin{center} \begin{tcolorbox}[title = #2] $$#3$$\\$$#4$$\\$$#5$$\end{tcolorbox}\end{center}}

\begin{document}
    \begin{center}
        \Large Math 307 Review Notes\\
        \normalsize Reese Critchlow
    \end{center}
    
    \section*{Midterm 1}


    \header{LU Decomposition}

    The gaussian elimination on a matrix $A$ can be expressed using \underline{LU Decomposition}.
    LU Decomposition follows the form:
    \[
        A = LU  
    \]
    Where $L$ is a \underline{unit lower triangular matrix} and $U$ is a \underline{upper triangular matrix}.
    \begin{multicols}{2}
        \centertext{\underline{Unit Lower Triangular Matricies}}

        A unit lower triangular matrix has the following attributes:
        \begin{itemize}
            \item The matrix is square $(n \times n)$.
            \item The diagonal entries of the matrix are ones and only zeroes are
            above the ones.
        \end{itemize}
        \[
            \begin{bmatrix}
                1 & 0 & 0 & 0 \\
                x & 1 & 0 & 0 \\
                x & x & 1 & 0 \\
                x & x & x & 1
            \end{bmatrix}
        \]
        \vfill\null\columnbreak
            
        \centertext{\underline{Upper Triangular Matricies}}
        An upper triangular matrix has the following attribute:
        \begin{itemize}
            \item The matrix has only zeroes below the main diagonal.
        \end{itemize}
        \[
            \begin{bmatrix}
                0 & x & x & x\\
                0 & 0 & x & x
            \end{bmatrix}  
        \]
        \vfill\null
    \end{multicols}
    It is also important to note that $L = E_1^{-1}E_2^{-1}E_3^{-1}$.
    \gap
    To find the LU decomposition:
    \begin{enumerate}
        \item Perform Gaussian elimination to obtain the row echelon form of the matrix,
        by using matrix multiplication, noting each $E$ along the way. The matrix in REF
        is the $U$ part of the LU decomposition.
        \item Compute the inverse of all of the $E$ matricies.
        \item Multiply the inverse matricies in reverse order together to obtain $L$. 
    \end{enumerate} 

    \header{Elementary Row Operations as Matrix Multiplications}

    The elementary row operations can be expressed as matrix multiplications. They
    are as follows:

    \begin{multicols}{3}
        \noindent
        \centertext{\underline{Interchange rows $i$ and $j$.}}
        Modify the identity matrix such that:
        \begin{align*}
            a_{i, i} = 0 && a_{j, j} = 0\\ a_{i, j} = 1 && a_{j, i} = 1
        \end{align*}
        \[
            P = \begin{bmatrix}
                1 & 0 & 0 & 0 & 0 & 0\\
                0 & 1 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 1_i & 0\\
                0 & 0 & 0 & 1 & 0 & 0\\
                0 & 0 & 1_j & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 1\\
            \end{bmatrix}  
        \]        
        \vfill\null\columnbreak
        
        \centertext{\underline{Multiply a row $i$ by a scalar $k$.}}
        Modify the identity matrix such that:
        \begin{align*}
            a_{i, i} = k
        \end{align*}
        \[
            D = \begin{bmatrix}
                1 & 0 & 0 & 0 & 0\\
                0 & 1 & 0 & 0 & 0\\
                0 & 0 & k_i & 0 & 0\\
                0 & 0 & 0 & 1 & 0\\
                0 & 0 & 0 & 0 & 1\\
            \end{bmatrix}
        \]
        \vfill\null\columnbreak

        \centertext{\underline{Add $c$ times row $j$ to $i$.}}
        Modify the identity matrix such that:
        \begin{align*}
            a_{i,j} = c
        \end{align*}
        \underline{Note:} it is required that $i > j$.
        \begin{align*}
            E = \begin{bmatrix}
                1 & 0 & 0 & 0\\
                0 & 1 & 0 & 0\\
                c_{i,j} & 0 & 1 & 0\\
                0 & 0 & 0 & 1\\
            \end{bmatrix}
        \end{align*}
        It is also important to note that the $E^{-1}$ can be acheived by modifying
        the identity matrix such that:
        \begin{align*}
            a_{j, i} = -c.
        \end{align*}
    \end{multicols}
    Hence, Gaussian elimination can be expressed as $E_3E_2E_1A$.
    \gap
    It is important to note that the order of the inverse is paramount. The inverse
    of $E_3E_2E_1$ is $E_1^{-1}E_2^{-1}E_3^{-1}$, not $E_3^{-1}E_2^{-1}E_1^{-1}$.
    In short, inverses are to be applied in the opposite order that the original
    matricies were applied.
    \gap
    Generally, only the $E$ transformation is used, because the rest of the transformations
    do not result in a lower triangular matrix.
    \gap
    \underline{Theorem:} If a matrix $A$ can be converted to row echelon form using
    \underline{only} $E$ row operations, then $A$ has an LU decomposition.
    \gap
    Sometimes, the LU decomposition is not attainable, thus, other transformations
    are permitted, but the LU decomposition will take on the form of $A=PLU$ where $P$
    is a permuatation matrix.
    \gap

    \underline{Important Properties of the LU Decomposition:}
    \begin{itemize}
        \item $\textrm{rank}(A) = \textrm{rank}(U)$
        \item If a is a square matrix, then: $\det(A) = \det(U)$
        If A is a square matrix of full rank:
        \item $\det(A) \neq 0$.
        \item $A$ is invertible.
        \item $A\vec{x} = \vec{b}$ has a unique solution.
    \end{itemize}

    \underline{Rank of a Matrix:} As a review, the \underline{rank} of a matrix is:
    \begin{itemize}
        \item The dimension of the span of the matrix.
        \item The number of non-zero leading/pivot entries in a matrix which is
                in row echelon form.
    \end{itemize}

    \underline{Row Echelon Form:} As a review, a matrix is considered to be in \underline{row echelon form}
    when 
    \begin{itemize}
        \item All rows consisting of only zeroes are at the bottom.
        \item The pivot entry of any nonzero row is always strictly to the right of the
            leading coefficient of the row above it.
    \end{itemize}

    \underline{Notes on Rank:}
    \begin{itemize}
        \item If $\textrm{rank}(A) = \textrm{rank}(A|B) =$ the number of rows in $\vec{x}$, then
        the system has a \underline{unique solution}.
        \item If $\textrm{rank}(A) = \textrm{rank}(A|B) <$ the number of rows in $\vec{x}$, then the system has \underline{infinite solutions}.
        \item If $\textrm{rank}(A) <$ the number of rows in $A$.
        \item \underline{Overarching theory:} $\dim(N(A)) = \textrm{cols}_A - \textrm{rank}(A)$.
    \end{itemize}

    \underline{Inverting a Matrix:} To invert a matrix $A$, create a amtrix $[A \mid I]$ and
    use row operations to transform the matrix to a form of $[I \mid A^{-1}]$.
\gap
\header{Error Analysis and Matrix Norms}

\underline{Matrix Norm:} The \underline{matrix norm} or operator norm is defined by:
\begin{align*}
    ||A|| = \max_{x \neq 0} \frac{||A\vec{x}||}{||\vec{x}||} && \textrm{where } ||\vec{x}|| \textrm{is the } e^2 \textrm{ norm}.
\end{align*}
The matrix norm describes the maximum stretch of a unit vector.
\underline{Inverse Matrix Norm}: If a matrix $A$ is a square, non-singular matrix, then
\[
    ||A^{-1}|| = \frac{1}{\min_{||\vec{x}|| = 1}||A\vec{x}||}.
\]
\underline{Significance of the Matrix Norm:} If a matrix or a vector is obtained
emperically, there may be errors in it. Thus, the matrix norm allows us to predict
how large the effect of those errors may be.
\gap
\underline{Condition Number:} The \underline{condition number} of a matrix $A$ is 
defined as:
\[
    \textrm{cond}(A) = ||A||\cdot||A^{-1}||
\]

\underline{Error Bounding:} Given a system $A\vec{x} = \vec{b}$, assuming that
$A(\vec{x} + \Delta\vec{x}) = \vec{b} + \Delta \vec{b}$, then:
\[
    \frac{||\Delta\vec{x}||}{||\vec{x}||} \leq \textrm{cond}(A) \cdot \frac{||\Delta \vec{b}||}{||\vec{b}||}.
\]
The equation above describes the error in $\vec{x}$ as a result of errors in $\vec{b}$.
\gap
One can also produce an expression for the effects of errors in $A$:
\[
    \frac{||\Delta \vec{x}||}{||\vec{x} + \Delta \vec{x}||} \leq \textrm{cond}(A) \cdot \frac{||\Delta A||}{||A||}  
\]
A common type of problem is to extract the condition number from the manipulation of
an image.

\underline{Norm For Diagonal Matricies:} The norm of a diagonal matrix $D$ can be obtained
with the following formula:
\[
    D = \max\left\{|d_k|\right\}
\]
where $d_k$ are the set of diagonal entries in the matrix.
\gap
\underline{Relations of the Condition Number:} It is important to note that:
\begin{align*}
    ||AB|| \leq ||A||\cdot||B|| && \textrm{thus} && \textrm{cond}(AB) \leq \textrm{cond}(A) \cdot \textrm{cond}(B)
\end{align*}

\header{Polynomial Interpolation}
A polynomial of the form:
\[
    P_\alpha = \left\{ c_0 + c_1t + c_2t^2 + \cdots + c_\alpha t^\alpha : c_n \in \mathbb{R}\right\}  
\]
Can be used to interpolate $\alpha$ number of points, so long as $t_i \neq t_j$, and $y_i \neq t_i$.
\gap
Given a system with multiple points we can solve a system of equations to obtain the 
unknonw coefficients, $c_n$.
\gap
Such a system of equations takes the form of $A\vec{c} = \vec{y}$, where $A$ is
known as the \underline{Vandermonde Matrix}.
\[
    \begin{bmatrix}
        1 & t_0 & t_0^2 & \cdots & t_0^\alpha\\
        1 & t_1 & t_1^2 & \cdots & t_1^\alpha\\
        1 & t_2 & t_2^2 & \cdots & t_2^\alpha\\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        1 & t_\alpha & t_\alpha^2 & \cdots & t_\alpha^\alpha\\
    \end{bmatrix}  
    \begin{bmatrix}
        c_0\\
        c_1\\
        c_2\\
        \vdots\\
        c_\alpha
    \end{bmatrix}
    =
    \begin{bmatrix}
        y_0\\
        y_1\\
        y_2\\
        \vdots\\
        y_\alpha
    \end{bmatrix}
\]
\underline{Determinant of a Vandermonde Matrix:} The determinant of a Vandermonde matrix
can be obtained by the following formula:
\[
    \det(A) = \prod_{0 \leq i < j \leq \alpha}(t_j - t_i)    
\]
It is important to note that in a Vandermonde matrix, the entries $t_0, t_1, \cdots, t_n$
represent time points and the $y$ vector represents their corresponding values.
\gap
It is also important to note that the condition number of a Vandermonde matrix gets
\textit{very} large when the number of points increases.
\gap
\header{Cubic Spline Interpolation}
Another method for interpolation is the \underline{cubic spline method}. Unlike the
continuous nature of the Polynomial Interpolation, the cubic spline method is 
piecewise.
\gap
By definition, a cubic spline interpolation has the following features/properties.
\begin{itemize}
    \item There are N cubic polynomials of the form:\\
    $
        p_k(t) = a_k(t - t_{k-1})^3 + b_k(t - t_{k-1})^2 + c_k(t - t_{k - 1}) + d_k    
    $
    \item $p(t)$, $p'(t)$, and $p''(t)$ are continuous
    \item $p(t_i) = y_i$ for all $i = 0, \cdots, N$
\end{itemize}
To find a cubic spline interpolation, one must use the coefficient matrix of a cubic
spline, which has the following form:
\[
    C = \begin{bmatrix}
        a_1 & a_2 & \cdots & a_N\\
        b_1 & b_2 & \cdots & b_N\\
        c_1 & c_2 & \cdots & c_N\\
        d_1 & d_2 & \cdots & d_N
    \end{bmatrix}    
\]
Hence, it can be concluded from this that every cubic spline interpolation coefficient
matrix will have $4N$ terms, where $N$ is the number of data points.
\gap
\underline{Equations Defining the Cubic Spline:} There exist several methods to obtain
the equations to solve for a cubic spline. They are as follows:
\begin{enumerate}
    \item Interpolation
    \begin{enumerate}
        \item Left Endpoints\\
        Basis: $p_k(t_{k-1}) = y_{k-1}$\\
        Result: $d_k = y_{k - 1}$\\
        Number of Equations: $N$
        \item Right Endpoints\\
        Basis: $p_k(t_k) = y_{k}$\\
        Result: $a_k(t_k - t_{k-1})^3 + b_k(t_k - t_{k-1})^2 + c_k(t_k - t_{k - 1}) + d_k = y_k$\\
        Number of Equations: $N$
    \end{enumerate}
    \item Continuity of the Derivative\\
    Basis: $p'_k(t_k) = p'_{k+1}(t_k)$\\
    Result: $3a_k(t_k - t_{k-1})^2 + 2b_k(t_k - t_{k - 1}) + c_k = c_{k + 1}$\\
    Number of Equations: $N-1$
    \item Continuity of the Second Derivative\\
    Basis: $p''_k(t_k) = p''_{k+1}(t_k)$\\
    Result: $6a_k(t_k - t_{k - 1}) + 2b_k = 2b_{k + 1}$\\
    Number of Equations: $N-1$
    \item Secondary Methods
    \begin{enumerate}
        \item Neutral Cubic Spline Conditions
        Equations: $p''(t_0) = 0$ and $p''_N(t_N) = 0$
    \end{enumerate}
\end{enumerate}

\underline{Obtaining the Coefficients for a Cubic Spline:}
First, it is known that $d_n = y_{n-1}$. Thus, the system that needs to be solved is
as follows:
\[
    \begin{bmatrix}
        A(L_1) & B & 0 \cdots 0\\
        0 & A(L_2) & B \cdots 0\\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        T & 0 & 0 & V
    \end{bmatrix}
    \begin{bmatrix}
        a_1\\
        b_1\\
        c_1\\
        a_2\\
        b_2\\
        c_2\\
        \vdots\\
        a_N\\
        b_N\\
        c_N
    \end{bmatrix}
    =
    \begin{bmatrix}
        y_1 - y_0 \\
        0\\
        0\\
        y_2 - y_1\\
        0\\
        0\\
        \vdots\\
        y_N - y_{N-1}\\
        0\\
        0
    \end{bmatrix}
\]
Where $L_k = t_k - t_{k-1}$ and:
\begin{align*}
    A(L) = \begin{bmatrix}
        L^3 & L^2 & L\\
        3L^2 & 2L & 1\\
        6L & 2 & 0
    \end{bmatrix}
    &&
    B = \begin{bmatrix}
        0 & 0 & 0\\
        0 & 0 & -1\\
        0 & -2 & 0
    \end{bmatrix}
    &&
    T = \begin{bmatrix}
        0 & 0 & 0\\
        0 & 2 & 0\\
        0 & 0 & 0
    \end{bmatrix}
    &&
    V = \begin{bmatrix}
        L_N^3 & L_N^2 & L_N \\
        0 & 0 & 0\\
        6L_N & 2 & 0
    \end{bmatrix}.
\end{align*}

\header{Subspaces}

\underline{Definition:} A subset $U \subseteq \mathbb{R}^n$ is a \underline{subspace} of
$\mathbb{R}^n$ under the following conditions:
\begin{enumerate}
    \item $U$ contains the zero vector $\vec{0} \in \mathbb{R}$.
    \item Closed under addition: $\vec{u_1}, \vec{u_2} \in U \Rightarrow \vec{u_1} + \vec{u_2} \in U$.
    \item Closed under scalar multiplication: $\vec{u} \in U, c \in \mathbb{R} \Rightarrow c\vec{u} \in U$.
\end{enumerate}

For example, the smallest subspace of $\mathbb{R}^2$ is $\{\vec{0}\}$, and the largest
subspace of $\mathbb{R}^2$ is simply $\mathbb{R}^2$. A common subspace however is
any line passing through the origin.
\gap
Subspaces of $\mathbb{R}^3$ include lines through the origin and planes containing
the origin.
\gap
\header{Span}
\underline{Definition:} The \underline{span} of a set of vectors is the set of all of
the possible inear combinations of them.
\gap
\underline{Determining Span Membership:} To determine whether a vector, $\vec{v}$ is within the
span of a set of vectors, $\{\vec{u}_n\}$, one can write $\begin{bmatrix}
    \vec{u_1} & \vec{u_2} & \cdots & \vec{u_n} \mid \vec{v}
\end{bmatrix}$ and solve the matrix.
\gap
\header{Linear Independence}
The vectors $\vec{u_1}, \cdots, \vec{u_m}\in \mathbb{R}$ are said to be
\underline{linearly independent} if $c_1\vec{u_1} + \cdots + c_m\vec{u_m} = \vec{0}$
if and only if the solution is trivial.
\gap
A more algorithmic approach to finding linear independence is to solve the matrix
$\begin{bmatrix}
    \vec{u_1} \cdots \vec{u_n} \mid \vec{0}
\end{bmatrix}$

\header{Basis and Dimension}

\underline{Definition:} Let $U \subseteq \mathbb{R}^n$ be a subspace. A set of vectors
$\{\vec{u_m}\}$ forms a \underline{basis} of $U$ if:
\begin{enumerate}
    \item $\{\vec{u_m}\}$ is a linearly independent set.
    \item $\textrm{span}\{\vec{u_m}\} = U$.
\end{enumerate}
\underline{Remark:} There are infinitley many different bases of a subspace $U$,
but each basis of $U$ has the same number of vectors.
\gap
\underline{Dimension:} The \underline{dimension} of $U$ is the number of vectors in a
basis of $U$. It is written as $\dim(U)$.
\gap
\underline{Finding Dimension and Span:}
Given a set of vectors $\{\vec{u_m}\}$, we can solve $\begin{bmatrix}
    \vec{u_1} \cdots \vec{u_m} \mid \vec{0}
\end{bmatrix}$
In row echelon form, redundant columns can be eliminated such that the associated
matrix in row echelon form is full rank. Thus, $\dim(U)$ is the rank of the matrix,
or the number of vectors in the span.
\gap
\header{Nullspace $N(A)$}

\underline{Definition:} Let $A$ be an $m\times n$ matrix. The \underline{nullspace} of $A$ is:
\[
    N(A) = \left\{\vec{x} \in \mathbb{R} : A\vec{x} = \vec{0}\right\} 
\]
In plain English, it is said that the nullspace is the set of vectors which by multiplication
of $A$ turn into the zero vector.
\gap
\underline{Finding the Nullspace:} The nullspace can be obtained by solving the matrix
$\begin{bmatrix}
    A \mid \vec{0}
\end{bmatrix}$, such that $A\vec{x} = \vec{0}$. The span of the solution to the matrix
is the nullspace. The number of vectors in the span is the dimension of the matrix.
\\
\pagebreak
\\
\header{Range $R(A)$}

\underline{Definition:} Let $A$ be an $m\times n$ matrix. The range of $A$ is:
\[
    R(A) = \left\{\vec{y} \in \mathbb{R} : A\vec{x} = \vec{y} \textrm{ for some } \vec{x} \in \mathbb{R}^M \right\}.
\]
\underline{Finding the Range:} Given a matrix $A$, use Gaussian elimination to bring
it into row echelon form. The columns in the original matrix who have pivot entries
in the REF matrix form the span of the range.
\gap
\header{Rank Nullity Theorem}
Let $U$ be the row echelon form of $A$. We can generalize that:
\gap
$\dim(R(A))$: Number of columns in $U$ \underline{with} a leading nonzero pivot element \underline{(rank of the matrix)}.
\gap
$\dim(N(A))$: Number of columns in $U$ \underline{without} a leading nonzero pivot element.
\gap
\underline{Theorem:} The \underline{rank-nullity theorem} states that $\dim(R(A)) + \dim(N(A)) = n$
for an $n \times m$ matrix. 
\gap
\header{Implications of the Rank-Nullity Theorem}
\underline{Theorem:} Let $A=LU$ be the LU decomposition of $A$ (if it exists),
and let $r = \textrm{rank}(A)$.
\smallskip\\
Then, $R(A) = \textrm{span}\{\vec{l_1}, \cdots, \vec{l_r} \}$ where $\vec{l_n}$ are
the first $r$ columns of $L$.

\pagebreak

\section*{Midterm 2}

\header{Inner Product}

The \underline{inner product} or \underline{dot product} has some important
properties:
\begin{itemize}
    \item If two vectors are orthogonal, the inner product is zero.
    \item If two vectors are colinear, their inner product is the product of the 
    magnitude of the two vectors.
\end{itemize}

\header{Orthogonal Stuff}
\sheader{Orthogonal Sets} A set is orthogonal if all vectors in the set 
are orthogonal to each other.
\gap
\sheader{Orthonormal Sets} A set is orthonormal if all vectors in the set 
are orthogonal to each other \textbf{and} the length of each vector is 1.
\gap
\sheader{Remark} If vectors are orthogonal, they are linearly independent. 
\gap
\header{Orthogonal Complements}
\sheader{Orthogonal Subspaces} Two subspaces, $S_1$ and $S_2$ are are considered to be 
\underline{orthogonal subspaces} if
\begin{align*}
    \forall \vec{x} \in S_1, \vec{y} \in S_2, \langle\vec{x}, \vec{y}\rangle = 0.
\end{align*}

\sheader{Orthogonal Complements} An orthogonal complement of 
a matrix $U$, $U^\perp$ is the set of vectors:
\begin{align*}
    U^\perp = \curly{\vec{x} \in \mathbb{R}^n : \langle\vec{x}, \vec{u}\rangle = 0, \forall \vec{u} \in U}. 
\end{align*}
\gap
\sheader{Remark} For a subspace $U \in \mathbb{R}^n$, $\dim(U) + \dim(U^\perp) = n$.
\gap
\sheader{Finding an Orthogonal Complement} The orthogonal complement is 
simply the null space of the matrix. Hence, $U^\perp = N(A)$.
\gap
\sheader{Properties of Orthogonal Complements}
\begin{align*}
    N(A) &= R(A^T)^\perp & R(A) &= N(A^T)^\perp\\
    N(A)^\perp &= R(A^T) & R(A)^\perp &= N(A^T)
\end{align*}

\header{Projections}

The projection of a vector $\vec{x}$ onto another vector $\vec{u}$ 
is given by:
\begin{align*}
    \textrm{proj}_{\vec{u}}(\vec{x}) = \frac{\langle \vec{x}, \vec{u}\rangle}{\langle \vec{u}, \vec{u}\rangle} \cdot \vec{u}.
\end{align*}
Vector projections can also be given by matrix multiplication:
\begin{align*}
    \textrm{proj}_{\vec{u}}(\vec{x}) = \frac{1}{\langle \vec{u}, \vec{u}\rangle} \vec{u}\vec{u}^T \vec{x}.
\end{align*}
Hence, we can also define a \underline{projection matrix} onto a 
vector, $\vec{u}$ as:
\begin{align*}
    P = \frac{1}{||\vec{u}||^2} \vec{u}\vec{u}^T
\end{align*}
The projection matrix has some interesting properties:
\begin{itemize}
    \item $P^T = P$ (symmetric)
    \item $P^2 = P$
    \item $\textrm{rank}(P) = 1$
\end{itemize}
\sheader{Projecting Vectors onto Subspaces} To project a vector onto
a subspace U, we define $\textrm{proj}_U(\vec{x})$ as a linear 
combination of projections of $\vec{x}$ onto basis vectors of U.
\gap
We also would like to have an \underline{orthogonal} basis of $U$,
such that:
\begin{align*}
    \vec{x} - \textrm{proj}_U(\vec{x}) \in U^\perp
\end{align*}

\sheader{Projecting onto the Orthogonal Complement}
Let $U$ be a subspace. The projection matrix $P^\perp$ onto the 
orthogonal complement of $U^\perp$ is given by:
\begin{align*}
    P^\perp = I - P.
\end{align*}
\gap
\sheader{Projection Theorem} From the projection, we can state that
\begin{align*}
    ||\vec{x} - \proj{U}{\vec{x}} || = ||\proj{U^\perp}{\vec{x}}||  \leq ||\vec{x} - \vec{u}||
\end{align*}
for $\vec{u} \in U$. In other words, the vector $\proj{U}{\vec{x}}$
is the vector in $U$ that is closest to $\vec{x}$.
\gap
\header{Constructing Orthogonal Bases}
A set of vectors $A$ is an \underline{orthogonal basis} of a set of vectors,
$U$, if 
\begin{enumerate}
    \item $A$ is a basis of $U$.
    \item $A$ is an orthogonal set of vectors.
\end{enumerate}

An important tool for constructing orthogonal bases is the 
\underline{Gram-Schmidt Algorithm}. For a set of vectors $\curly{\vec{u_1}, \ldots, \vec{u_m}}$,
a basis of $U \subseteq \mathbb{R}^n$, the Gram-Schmidt Algorithm produces
the following vectors:
\begin{align*}
    \vec{v}_1 &= \vec{u}_1\\
    \vec{v}_2 &= \vec{u}_2 - \proj{\vec{v}_1}{\vec{u}_2}\\
    \vec{v}_3 &= \vec{u}_3 - \proj{\vec{v}_1}{\vec{u}_3} - \proj{\vec{v}_2}{\vec{u}_3}\\
    &\vdots\\
    \vec{v}_m &= \vec{u}_m - \proj{\vec{v}_1}{\vec{u}_m} - \cdots - \proj{\vec{v}_{m - 1}}{\vec{u}_m}
\end{align*}
Hence, it is implied that $\curly{\vec{v}_1, \ldots, \vec{v}_m}$
is an orthogonal basis of $U$.
\gap
Furthermore, we can also define
\begin{align*}
    \vec{w}_i = \frac{\vec{v}_i}{||\vec{v}_i||} & i = 1, \ldots, m
\end{align*}
Thus, we get that $\curly{\vec{w}_1 , \ldots, \vec{w}_m}$ is an 
\underline{orthnormal basis} of $U$.

\pagebreak

\header{QR Decompositions}

A QR decomposition on an $m \times n$ matrix with $\textrm{rank}(A) = n$
gives an alternate representation $A = QR$, which contains orthonormal
bases of $R(A)$ and $R(A)^\perp$. This can be seen in $Q = [Q_1 Q_2]$,
where $Q_1$ is the first $n$ columns of $Q$, and is the orthonormal
basis of $R(A)$, and the remaining columns form an orthonormal basis
of $R(A)^\perp$. 
\gap
It is also to be noted that $Q$ is an orthogonal matrix with the same
dimensions of the original matrix $(m \times n)$ and $R$ is an upper
triangular matrix with the same dimensions of the original matrix.
\gap
\sheader{Finding the QR Decomposition}
\begin{enumerate}
    \item Obtain $R(A)$ and $R(A)^\perp$
    \item Use the Gram-Schmidt algorithm to get an orthogonal basis
    of both $R(A)$ and $R(A)^\perp$, $\curly{\vec{w}}$. These form $Q_1$ and $Q_2$, respectivley.
    \item Construct $R_1$ given the following formula:
    \begin{align*}
        \begin{bmatrix}
            \langle \vec{w}_1, \vec{a}_1 \rangle & \langle \vec{w}_1, \vec{a}_2 \rangle & \cdots & \langle \vec{w}_1, \vec{a}_1 \rangle \\
            0 & \langle \vec{w}_2, \vec{a}_2 \rangle & \cdots & \vdots \\
            0 & 0 & \cdots & \vdots \\
            \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & \cdots & \langle \vec{w}_n, \vec{a}_n \rangle
        \end{bmatrix}
    \end{align*}
    \item For $R_2$, expand $R_1$ with zeroes until its dimensions
    are $m \times n$.
\end{enumerate}

Just finding $Q_1$ and $R_1$ such that $A = Q_1 R_1$ is called the 
\underline{Thin QR Decomposition} of the matrix $A$.
\gap
\header{Least Squares Approximations}

The \underline{least squares approximation} is used for finding an exact solution 
for a system that has infinite solutions. In this, we seek to minimize $||A\vec{x} - \vec{b}||$.
\gap
By the \underline{projection theorem}, we can visualize the least squares approximation
as finding the vector in the range of our system $R(A)$ which is closest to $\vec{b}$.
This is the orthogonal projection of $\vec{b}$ onto $R(A)$.
\gap
There exist two ways in which we can find a least squares solution to a problem:
\begin{enumerate}
    \item \sheader{Normal Equations} The least squares approximation to $A\vec{x} = \vec{b}$
    is the solution to the \underline{normal equations} system $A^TA\vec{x} = A^T \vec{b}$.
    \item \sheader{QR Decompositions} The least squares approximation to $A\vec{x} = \vec{b}$
    is simply $R_1\vec{x} = Q_1^T\vec{b}$.
\end{enumerate}
\sheader{Remark} The QR decomposition method \textit{apparently} has a lower condition number.
\gap
\header{Eigenvalues and Eigenvectors}
Most of this is review from earlier linear algebra courses but, there exist some 
important points:
\begin{itemize}
    \item An eigenvalue is called \underline{defective} if it is repeated in 
    the characteristic equation.
\end{itemize}
\pagebreak
\header{Diagonalization}
Another method for matrix decomposition is diagonalization. A matrix $A$ is diagonalizable
if you can bring it into the form:
\begin{align*}
    A = PDP^{-1}
\end{align*}
Where for eigenvectors of the matrix $A$ are $\vec{v}_k$ and eigenvalues for corresponding
eigenvectors are $\lambda_k$, and
\begin{align*}
    P = [\vec{v}_1 \cdots \vec{v}_n] && D = \begin{bmatrix}
        \lambda_1 & & \\
         & \ddots & \\
        & & \lambda_n
    \end{bmatrix}.
\end{align*}
Diagonalizable matricies have a special subcase, which is outlined in the
\underline{spectral theorem}. The spectral theorem states that:
For a real, symmetric matrix $A$, there exists an orthogonal matrix $P$ and a 
diagonal matrix $D$ such that:
\begin{align*}
    A = PDP^T.
\end{align*}
Where $P$ and $D$ are defined in the same way as a normal diagonalizable matrix.
\gap
\header{Singular Value Decomposition}
Another decomposition that we can use is the \underline{singular value decomposition}
The singular value decomposition, or SVD, is similar to a diagonalization, but is
different in some ways. Its form is given as:
\begin{align*}
    A = P\Sigma Q^T 
\end{align*}
Where a \underline{singular value} $\sigma_k = \sqrt{\lambda_k}$, for $\lambda_k$
being an eigenvalue of the matrix $AA^T$, and where $AA^T$ has $r$ eigenvalues. Hence,
we can describe each matrix as the following:
\begin{align*}
    P = [\vec{p}_1 \cdots \vec{p}_r \, \, \vec{p}_{r + 1} \cdots \vec{p}_m]_{m \times n}
&&
    \vec{p}_k = \begin{cases}
        \frac{1}{\sigma_k} A \vec{q}_k & 0 \leq k \leq r\\
        \textrm{orthonormal basis of }N(AA^T) & r < k \leq m
    \end{cases}
\end{align*}

\begin{multicols}{2}
    \begin{align*}
        \Sigma = \begin{bmatrix}
            \sigma_1 & & & 0\\
            & \ddots & & \vdots \\
            & & \sigma_r & 0\\
            0 & \cdots & 0 & 0
        \end{bmatrix}_{m \times n}
    \end{align*}
    Where $\sigma_1$ is the largest singular value, and $\sigma_r$ is the smallest singular value.
    \vfill\null\columnbreak
    \begin{align*}
        Q = [\vec{q}_1 \cdots \vec{q}_r \, \, \vec{0} \cdots \vec{0}]_{m \times n}
    \end{align*}
    \begin{align*}
        A^TA \vec{q}_k = \sigma_k^2 \vec{q}_k
    \end{align*}
    \vfill\null
\end{multicols}
$Q$ can also be characterized as a matrix containing the eigenvectors of $AA^T$,
in order of smallest to largest corresponding eigenvalue. However, these eigenvectors 
must be given as an orthonormal basis. 
\gap
The ``extension vectors'' of $P$ $(\vec{p}_k, k > r)$ can also be characterized
as the vectors in the nullspace of $(\vec{p}_k, k \leq r)$. 
\gap
It is important to note that the elements of a SVD can be characterized as the
following:
\begin{itemize}
    \item $P$: Rotation/Reflection
    \item $\Sigma$: Stretch
    \item $Q$: Rotation/Reflection
\end{itemize}
Hence, the singular values provide us with important information on the norm 
and condition number of the matrix. 
\gap
Thus, it can be said that $||A|| = \sigma_1$ and $||A^{-1}|| = \frac{1}{\sigma_n}$,
so $\textrm{cond}(A) = \frac{\sigma_1}{\sigma_n}$.

\pagebreak

\section*{Final Exam}

\header{Principle Component Analysis}
Some matricies do not have inverses. Thus, we introduce the concept of a \underline{pseudoinverse},
which builds off of the SVD to make something that's ``good enough''.
Given some data matrix $X$ which has dimensions $n \times p$,
comprised of a set of vectors $\vec{x_1}, \ldots, \vec{x_n} \in \mathbb{R}^p$, assuming that the 
data is normalized such that 
\begin{align*}
    \sum_{k = 1}^n \vec{x_n} = \vec{0}.
\end{align*}
The \underline{Principle Component Analysis} gives the vectors $\vec{w_q}, \ldots, \vec{w_n}$
Where $\vec{w}_1$ is the vector that maximizes $\sum_{k = 1}^n ||\proj{\vec{w_1}}{\vec{x_k}}||^2 = ||X\vec{w_1}||^2$
\gap
More generally, we can say that with the set of weight vectors $\curly{\vec{w_k}}$,
the $k$-th weight vector $\vec{w}_k$ is the unit vector which maximizes $||X_k \vec{w}_k||$,
Where $X_k$ is the projection of the data matrix onto $\textrm{span}\curly{\vec{w}_1, \ldots, \vec{w}_{k - 1}}$.
This can also be written as:
\begin{align*}
    X_k = X - \sum_{i = 1}^{k = 1} X\vec{w}_i \vec{w}_i^T
\end{align*}
Hence, we can define the weight vectors as the \underline{right singular vectors of $X$}, 
which are the columns of the $Q$ matrix in the singular value decomposition.
\begin{align*}
    \vec{w}_k = \vec{q}_k.
\end{align*}
It's also important to note that each vector $\vec{w}_k$ is part of an orthonormal basis,
thus for each $\vec{w}_k$, $||\vec{w}_k|| = 1$.
\gap
\header{Pseudoinverses}

Given an $m \times n$ matrix $A$, with SVD $A = P\Sigma Q^T$, the \underline{pseudoinverse}
of $A$, denoted $A^+$ is:
\begin{align*}
    A^+ = Q \Sigma^+ P^T
\end{align*}
Where $\Sigma^+$ is given by:
\begin{align*}
    \Sigma^+ = \begin{bmatrix}
        \sigma_1^{-1} & & & \vdots\\
         & \ddots & & 0\\
         & & \sigma_r^{-1} & \vdots\\
         \cdots & 0 & \cdots & 0  
    \end{bmatrix}_{n \times m}
\end{align*}
We can paraphrase this as $\Sigma^+$ being $\Sigma$, but with every singular value its
reciprocal.
\gap
Thus, if $A$ is invertible, then $A^+ = A^{-1}$.
\gap
The pseudoinverse has some interesting properties:
\begin{align*}
    AA^+A = A && A^+ A A^+ = A^+.
\end{align*}

The pseudoinverse also provides a new way to compute a least squares approximation:
\begin{align*}
    \vec{x} = A^+ \vec{b}.
\end{align*}
An interesting implication of all things that we've done with the SVD is that we can take only
the largest singular values in a SVD and get a result that has minimal data loss compared 
to our original matrix.
\pagebreak

\header{Complex Numbers}

A complex number is of the form $z = a + ib$, where $a,b \in \mathbb{R}$ and $i = \sqrt{-1}$.
\gap
We can define some properties of a complex number $z$:
\begin{itemize}
    \item \underline{Real Part} The real part of $z$, is $a$, denoted $\textrm{Re}(z) = a$.
    \item \underline{Imaginary Part} The imaginary part of $z$, is $b$, denoted $\textrm{Im}(z) = b$.
    \item \underline{Modulus} The modulus, or length of $z$ is given by $|z| = \sqrt{a^2 + b^2} = r$.
    \item \underline{Argument} The argument, or angle of $z$ is given by $\arg(z) = \arctan\left(\frac{b}{a}\right) = \theta$.
\end{itemize}
Given these definitions, we can express complex numbers in different ways:
\begin{itemize}
    \item \sheader{Euler's Formula} $z = r \cos \theta + i r \sin \theta = r(\cos \theta + i \sin \theta)$.
    \item \sheader{Polar Form} $z = re^{i \theta}$
\end{itemize}
We can also define the \underline{conjugate} of $z$, denoted by $\overline{z}$ or $z^*$. 
\begin{align*}
    \overline{z} = a - ib = re^{-i \theta}.
\end{align*}

\header{Complex Vectors}
Building off of the work done with complex numbers, we can now define \underline{complex vectors}.
A complex vector $\vec{v} \in \mathbb{C}^n$ is a vector with $n$ complex entries.
\gap

\sheader{Conjugates of Complex Vectors} We can define the conjugate of a complex vector, which for some complex vector $\vec{v}$
is given by:
\begin{align*}
    \vec{v^*} = \begin{bmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{bmatrix}^*
    = \begin{bmatrix}
        v_1^*\\
        \vdots\\
        v_n^*
    \end{bmatrix}
\end{align*}
To paraphrase, a conjugate of a complex vector is just the original matrix with all of 
its entries conjugated.
\gap
\sheader{Complex Inner Product} We can also define the inner product for two complex vectors
$\vec{v}, \vec{w} \in \mathbb{C}^n$:
\begin{align*}
    \langle \vec{v}, \vec{w} \rangle = \vec{v_1^T} \vec{w^*} = \langle \vec{v}, \vec{w^*} \rangle.
\end{align*}
The complex inner product has some nice properties too:
\begin{itemize}
    \item $\langle c\vec{v}, \vec{w} \rangle = c \langle \vec{v}, \vec{w} \rangle$
    \item $\langle \vec{v}, c\vec{w} \rangle = c \langle \vec{v}, \vec{w} \rangle$
    \item $\langle \vec{v}, \vec{w} \rangle = \langle \vec{w}, \vec{v} \rangle^*$
    \item $\langle \vec{v}, \vec{v} \rangle = |v_1|^2 + \cdots + |v_n|^2$.
\end{itemize}
Hence, with the last property, we can say that the norm of a complex vector can be expressed by:
\begin{align*}
    ||\vec{v}|| = \sqrt{\langle \vec{v}, \vec{v} \rangle}
\end{align*}
We can also define another property for an $m \times n$ complex matrix, $A$, 
and two complex vectors $\vec{v} \in \mathbb{C}^m, \vec{w} \in \mathbb{C}^n$:
\begin{align*}
    \langle A\vec{v}, \vec{w} \rangle = \langle \vec{v}, A^{T*} \vec{w} \rangle.
\end{align*}
\pagebreak

\sheader{Hermetian Matricies} We call a complex matrix \underline{Hermetian} if $A = A^{T*} = \overline{A^T}$
\gap
If $A$ is Hermetian, then:
\begin{itemize}
    \item $\langle A\vec{x}, \vec{y} \rangle = \langle \vec{x}, A\vec{y} \rangle$.
    \item $A$ has only real eignevalues and $A$ is diagonalizable.
    \item The diagonal entries of $A$ are real numbers.
\end{itemize}

\header{Roots of Unity}
We define a complex number $\omega in \mathbb{C}$ as an $N$th root of unity if $\omega^N = 1$.\\
Generally, these are given in the form $\omega_n = e^{i \frac{2\pi}{N}}$.
\gap
Hence, we can define some important properties on roots of unity $\omega$.
\begin{enumerate}
    \item $\omega_N^N = 1$
    \item $\omega_N^{N - 1} = \overline{\omega_N} = \omega_N^*$
    \item $\overline{\omega_N} = \omega_N^{-1}$
    \item $\sum_{k = 0}^{N - 1}\omega_N^{kl} = 0, 0 < l < N$
\end{enumerate}

\header{Fourier Bases}
As seen prior, a \underline{standard basis} of $\mathbb{C}^n$ is $\vec{e}_0, \ldots, \vec{e}_{N-1}$ where $\vec{c}_k$
is given by:
\begin{align*}
    \vec{e}_k = \begin{bmatrix}
        0 \\
        \vdots\\
        0\\
        1\\
        0\\
        \vdots\\
        0
    \end{bmatrix}
\end{align*}
Where the $1$ is at entry $k$.
\gap
However, now we introduce the concept of a \underline{Fourier basis} of $\mathbb{C}^n$,
given by $\vec{f}_0, \ldots, \vec{f}_{N - 1}$, where 
\begin{align*}
    \vec{f}_k = \begin{bmatrix}
        1 \\ 
        \omega_N^k\\
        \omega_N^2k\\
        \vdots \\
        \omega_N^{(N-1)k}
    \end{bmatrix}
\end{align*}
Where $\omega_N = e^{i \frac{2\pi}{N}}$, and $n = N$.\gap
\sheader{Remark} A Fourier basis is an orthogonal basis of $\mathbb{C}^n$, where 
\begin{align*}
    \langle \vec{f}_k, \vec{f}_n \rangle = \begin{cases}
        N & k = n\\
        0 & k = n
    \end{cases}
\end{align*}
\sheader{Remark} $\vec{f^*_k} = \vec{f}_{N - k}$ for $0 < k < N$.

\pagebreak

\header{Discrete Fourier Transform}
The Discrete Fourier Transform, or DFT, is the vector coefficients of $\vec{x}$ 
with respect to a Fourier basis. Hence, we can define the DFT as:
\begin{align*}
    \textrm{DFT}(\vec{x}) = F_N \vec{x} = \begin{bmatrix}
        \vec{f^*_0}^T \\
        \vdots \\
        \vec{f^*_{N-1}}^T
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        \vec{f_0}^T\\
        \vec{f_{N-1}}^T\\
        \vdots \\
        \vec{f_1}^T
    \end{bmatrix}
\end{align*}
Due to the properties of complex conjugates, a nice symmetry emerges from a Fourier transform.
Given some $\vec{x} \in \mathbb{R}^n$ and $\vec{y} = \textrm{DFT}(\vec{x})$, then 
$\vec{y}[k] = \vec{y}[N - k]$ for $1 \leq k \leq N - 1$ or $0 < k < N$.
\gap
Finally, we can also reconstruct $\vec{x}$ from $\textrm{DFT}(\vec{x})$, since $F_N$ 
is invertible. Thus, we can obtain the inverse of $F_N$ as 
\begin{align*}
    F_N^{-1} = \frac{1}{N}{F_N^*}^T.
\end{align*}
Hence, the \underline{inverse discrete Fourier transform} of $\vec{y} \in \mathbb{C}^N$
is:
\begin{align*}
    \textrm{IDFT}(\vec{y}) = \frac{1}{N}{F_N^*}^T \vec{y}.
\end{align*}

\header{Sinusoids}
If we divide a time range $N$ into discrete time points such that:
\begin{align*}
    \vec{t} = \begin{bmatrix}
        0\\
        \frac{1}{N}\\
        \frac{2}{N}\\
        \vdots \\
        \frac{N - 1}{N}
    \end{bmatrix}.
\end{align*}
Hence, we can also use this to turn functions into discretized vectors. For example, given 
a function $f(x)$, then $f(\vec{t})$ is given by:
\begin{align*}
    f(\vec{t}) = 
    \begin{bmatrix}
        f(0)\\
        f\left(\frac{1}{N}\right)\\
        f\left(\frac{2}{N}\right)\\
        \vdots\\
        f\left(\frac{N-1}{N}\right)
    \end{bmatrix}.
\end{align*}
Hence for a sinusoid of the form:
\begin{align*}
    f(t) = A\cos(2\pi k\vec{t} + \phi)
\end{align*}
Where:
\begin{itemize}
    \item $A$ is the \underline{amplitude}.
    \item $k$ is the \underline{frequency}.
    \item $\phi$ is the \underline{phase shift}.
\end{itemize}
We can express sinusoids using the Fourier Basis:
\begin{itemize}
    \item $\vec{f_k} = \cos(2 \pi k \vec{t}) + i \sin (2 \pi k \vec{t})$
    \item $\frac{1}{2}\left(\vec{f_k} + {\vec{f_k}}^* \right) = \cos(2\pi k \vec{t})$
    \item $\frac{1}{2i}\left(\vec{f_k} - \vec{f_k}^*\right) = \sin(2\pi k\vec{t})$
\end{itemize}
Finally, we can define the Discrete Fourier transform for a cosine function:
\begin{align*}
    \textrm{DFT}(A \cos (2\pi k \vec{t} + \phi)) = \frac{AN}{2}e^{i\phi}\vec{e}_k + \frac{AN}{2}e^{-i\phi}\vec{e}_{N - k}
\end{align*}

\header{Stemplots and Reconstructions}

Given a Discrete Fourier Tranform output, $\vec{y} \in \mathbb{C}^n$, we can obtain a plot
of it's frequencies in the form of a \underline{stemplot}. To obtain a stemplot, we 
divide the modulus of each entry by $N$ if the entry is at index 0 or $\frac{N}{2}$, and if it is not
at either of these entries, then we divide the modulus by $\frac{N}{2}$. This completes 
a magnitude stemplot. For a angle plot, we plot the argument of each entry. If the entry is 
positive and real, the argument or angle is 0. If the entry is negative and real, then 
the argument is $\pi$.

In order to reconstruct the signal as a sum of sinusoids, we can take every entry of the 
Fourier transform and take the stemplot amplitude as the amplitude of the signal $A$, the 
index of the entry $k$ as the frequency, and the argument $\phi$ as the phase shift.
Hence, we can reconstruct the signal as a sum of cosines. An important remark is that 
we shall only do this up until the $N/2$th entry.

\end{document}