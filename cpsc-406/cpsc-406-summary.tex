\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper, portrait, margin=1in]{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\setlength\parindent{0pt}
\usepackage{enumerate}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{fancyhdr}
\usepackage{tcolorbox}
\hyphenchar\font=-1
\usepackage{tabularx}

\newcommand{\header}[1]{\begin{large}\noindent #1\end{large}\\\rule{\textwidth}{0.5pt}}
\newcommand{\mheader}[1]{\textbf{#1}\\\rule{0.33\textwidth}{0.5pt}\\}
\newcommand{\norm}[2]{\left\lvert\left\lvert#1\right\rvert\right\rvert}
\newcommand{\sheader}[1]{\underline{#1:}}
\newcommand{\gap}{\medskip\\}
\newcommand{\centertext}[1]{\begin{center}#1\end{center}}
\newcommand{\bfrac}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\formula}[3]{\begin{center} \begin{tcolorbox}[title = #2] $$#3$$\end{tcolorbox}\end{center}}
\newcommand{\where}{\hspace{0.5cm} \textrm{where} \hspace{0.5cm}}
\newcommand{\hgap}{\hspace{0.5cm}}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\doubleformula}[4]{\begin{center} \begin{tcolorbox}[title = #2] $$#3$$\\$$#4$$\end{tcolorbox}\end{center}}
\newcommand{\curly}[1]{\left\{#1\right\}}
\newcommand{\proj}[2]{{}\textrm{proj}_{#1}\left(#2\right)}
\newcommand{\sgap}{\smallskip\\}
\newcommand{\range}{\textrm{range}}

\newcommand{\ds}{\displaystyle}
\newcommand{\Arg}{\textrm{Arg}}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\newcommand{\oper}[2]{\underset{#2}{\operatorname{#1}}}




\newcommand{\tripleformula}[5]{\begin{center} \begin{tcolorbox}[title = #2] $$#3$$\\$$#4$$\\$$#5$$\end{tcolorbox}\end{center}}

\usepackage{physics}
% \usepackage{braket}

\begin{document}
\begin{center}
        \Large CPSC 406 Review Notes\\
        \normalsize Reese Critchlow\\
        \textit{Any Distribution for Commercial Use Without the Expressed Consent of the Creator is Strictly Forbidden.}
\end{center}

\header{Linear Algebra Review}
\sheader{Orthogonality} A $n \times m$ matrix is said to be \underline{orthonormal} if its columns are pairwise orthonormal:
\begin{align*}
    Q = [q_1 | \cdots | q_m].
\end{align*}
It follows that then, for an orthonormal matrix, then:
\begin{align*}
    Q^TQ = I_m.
\end{align*}
In addition, if $n = m$, that is if $Q$ is square, then $Q$ is said to be \underline{orthogonal}.
From this, it follows that:
\begin{align*}
    Q^{-1} = Q^T && Q^TQ = QQ^T = I_n.
\end{align*}
It is to be also noted that orthogonal transformations preserve lengths and angles.
\gap
\sheader{Nonsingularity} A matrix is said to be \textbf{nonsingular} if it has a matrix inverse and 
is square. A square matrix is nonsingular iff its determinant is nonzero.
\gap
\sheader{Condition Number} The condition number of an $n \times n$ positive definite matrix
$H$ is 
\begin{align*}
    \kappa(H) = \frac{\lambda_{\max} (H)}{\lambda_{\min} (H)} \geq 1.
\end{align*}
It is said that a matrix is \underline{ill-conditioned} if $\kappa(H) \gg 1$. If $f$
is twice continuously differentiable, the condition number of $f$ at solution $x^*$ is 
given by:
\begin{align*}
    \kappa(f) = \kappa(\nabla^2 f(x^*))
\end{align*}
\gap
\header{Linear Least Squares}

A basic linear least squares problem has the form:
\begin{align*}
    \min_x \norm*{Ax - b}^2_2.
\end{align*}
In essence, linear least squares seeks to find the vector $x$ that, when multiplied by 
the matrix $A$, returns the closest result to $b$. The $ij$-th entry of the $A$ matrix can be 
interpreted as the $i$-th observation of the $j$-th independent variable.
\gap
\sheader{Normal Equations}
We can write the least squares problem as a function $f$ of $x$, thus, the solution to the least 
squares problem must be a stationery point of $f$:
\begin{align*}
    x^* = \textrm{arg}\min_{x} f(x) := \frac{1}{2} \norm*{Ax-b}^2_2\\
    \implies \nabla f(x) = A^T - A^Tb\\
    \nabla f(x^*) = 0 \iff A^TAx^* - A^Tb = 0 \iff A^TAx^* = A^Tb
\end{align*}
If $A$ is full rank, then the solution $x^*$ is unique.
\gap
\sheader{Geometric Interpretation}
If the $n \times m$ matrix $A$ has range $\textrm{range}(A)$, and for some vector $b \in \mathbb{R}^m$
then the vector $Ax^*$, where $x^*$ is the solution to the least squares problem is the 
projection of $b$ onto the $\textrm{range}(A)$. Hence, we can also define the residual $r$ to be 
vector which is the difference between $Ax^*$ and $b$, $r = b - Ax^*$.\\
\smallskip
Hence, it must be that $r \in \textrm{null}(A^T)$, such that $A^Tr=0$.
\gap
\header{QR Factorization}
We can obtain the QR factorization for an $m \times n$ matrix, with $m > n$:
\begin{align*}
    A = [Q_1 | Q_2] \begin{bmatrix}
        R_1\\
        0
    \end{bmatrix}
\end{align*}
Where: 
\begin{itemize}
    \item $Q$ is orthogonal
    \item $R_1$ is [right] upper triangular
    \item $\textrm{range}(Q_1) = \textrm{range}(A)$
    \item $\textrm{range}(Q_2) = \textrm{range}(A)^\perp \equiv \textrm{null}(A^T)$.
\end{itemize}
We can use the QR factorization to solve $n \times n$ nonsingular matricies:
\begin{align*}
    x = A^{-1}b = R^{-1}Q^Tb
\end{align*}
This can also be used to solve least squares problems. Due to the condition number, it is said that
QR is a more numerically stable solution rather than the normal equations approach.
\gap
\header{Singular Value Decomposition (SVD)}
For any $m \times n$ matrix $A$ with rank $r$: 
\begin{align*}
    A = U\Sigma V^T = [u_1 | u_2 | \cdots | u_r] \begin{bmatrix}
        \sigma_1 & & &\\
        & \sigma_2 & & \\
        & & \ddots & \\
        & & & \sigma_r 
    \end{bmatrix}
    \begin{bmatrix}
        v_1^T \\
        v_2^T \\
        \vdots \\
        v_r^T \\
    \end{bmatrix}
\end{align*}
A simple interpretation of the components of the SVD are as follows:
\begin{itemize}
    \item $U$ is a basis for $\textrm{range}(A)$
    \item $V$ is a basis for $\textrm{range}(A^T)$
    \item $\Sigma$ contains all of the roots of the eigenvalues of $A$.
\end{itemize}
It is also nice to define two other norms for matricies given the SVD:
\begin{itemize}
    \item Spectral Norm: $\ds \norm*{A}_2 = \max_{\norm*{x}_2 = 1}\norm*{Ax}_2 = \sigma_1$
    \item Frobenious Norm: $\ds \norm*{A}_F = \sqrt{\textrm{trace}(A^TA)}= \sqrt{\sum_{i=1}^{r}\sigma_i^2}$
\end{itemize}
We can say that the SVD decomposes any matrix $A$ with rank $r$ into a sum of rank-1 matricies. Hence,
we can describe the best rank-k approximation by the following:
\begin{align*}
    A_k = \sum_{j = 1}^{k} \sigma_j u_j v_j^T.
\end{align*}
We can also say that the full SVD provides orthogonal bases for all four fundamental subspaces for an $m \times n$ matrix:
\begin{itemize}
    \item $\range(A) = \textrm{span}\curly{u_1, \ldots, u_r}$
    \item $\textrm{null}(A^T) = \textrm{span}\curly{u_{r + 1}, \ldots, u_m}$
    \item $\range(A^T) = \textrm{span}\curly{v_1, \ldots, v_r}$
    \item $\textrm{null}(A) = \textrm{span}\curly{v_{r + 1}, \ldots, v_n}$
\end{itemize}

\sheader{Minimum norm least-squares solution} Building off of the prior result of the fundamental 
subspaces, we obtain that:
\begin{align*}
    \overline{x} = Vy = \sum_{r}^{j=1}\frac{u_j^T b}{\sigma_j}v_j, \,\,\, \sigma_j y_j = \begin{cases}
        \overline{b}_j/ \sigma_j & j = 1:r \\
        0 & j = r + 1 : n
    \end{cases}
\end{align*}
\pagebreak

\header{Regularized Least Squares}
Regularized Least Squares is motivated by multi-objective optimization problems, where 
one must choose some $x$ to minimize $f_1(x)$ and $f_2(x)$, but they do not get small together.
Commonly, the solution space is divided into two parts, one containing possible solutions, and 
one containing impossible solutions. The boundary between these two sets is called the \underline{Pareto Frontier}.
\gap
\sheader{Weighted-Sum Objective} Commonly, the approach to a multi-objective optimization is 
to weight the sum of objectives:
\begin{align*}
    \min_{x} \alpha_1f_1(x) + \alpha_2f_2(x)
\end{align*}
Hence, the negative ratio of the two $\alpha$s ends up becoming the slope of the Pareto Frontier
at each given solution point on the curve $-\left(\frac{\alpha_1}{\alpha_2}\right)$.
\gap
\sheader{Tikhonov Regularization/Ridge Regression} This form of the least squares problem 
is generally employed for the case that the standard least-squares problem is ill-posed, and 
thus requires some sort of bias. It can also be applied for a case in which one would like 
to have a solution with particular characteristics be favoured. It is as follows:
\begin{align*}
    \min_x \frac{1}{2} \norm*{Ax - b}^2 + \frac{1}{2} \lambda \norm*{Dx}^2
\end{align*}
Where:
\begin{itemize}
    \item $\norm*{Dx^2}$ is the regularization penalty (often $D = I$)
    \item $\lambda$ is the positive regularization parameter
\end{itemize}
Hence, we can say that an equivalent expression for the objective is:
\begin{align*}
    \norm*{Ax - b}^2 + \lambda\norm*{Dx}^2 = \norm{\begin{bmatrix}
        A \\
        \sqrt{\lambda}D
    \end{bmatrix}x - \begin{bmatrix}
        b\\
        0
    \end{bmatrix}}^2.
\end{align*}
Hence, the normal equations then become:
\begin{align*}
    (A^TA + \lambda D^T D)x = A^T b.
\end{align*}

\header{Gradients, Linearizations, and Optimality}
For some function $f: \mathbb{R}^n \to \mathbb{R}$, then $x^* \in \mathbb{R}^n$ is a:
\begin{itemize}
    \item \textbf{Global Minimizer} if $f(x^*) \leq f(x), \,\, \forall x$.
    \item \textbf{Strict Global Minimizer} if $f(x^*) \leq f(x), \,\, \forall x$.
    \item \textbf{Local Minimizer} if $f(x^*) \leq f(x), \,\, \forall x \in \epsilon \mathbf{B}(x^*)$.
    \item \textbf{Strict Local Minimizer} if $f(X^*) < f(x), \,\, \forall x \in \epsilon \mathbf{B}(x^*)$.
\end{itemize}

\sheader{1-Dimensional Optimization} Standard Calc 1 definitions:
\begin{itemize}
    \item \textbf{Local Minimizer}: $f'(x) = 0 \wedge f''(x) > 0$
    \item \textbf{Local Maximizer}: $f'(x) = 0 \wedge f''(x) < 0$
\end{itemize}

\mheader{Multidimensional Optimization}\\
\sheader{Directional Derivatives} A directional derivative of $f: \mathbb{R}^n \to \mathbb{R}$ 
at $x \in \mathbb{R}^n$ in the direction $d \in \mathbb{R}^n$ is:
\begin{align*}
    f'(x;d) = \lim_{\alpha \to 0^+} \frac{f(x + \alpha d) - f(x)}{\alpha}
\end{align*}
\sheader{Descent Directions} A nonzero vector $d$ is a descent direction of $f$ at $x$ if:
\begin{align*}
    f(x + \alpha d) < f(x), \,\, \forall \alpha \in (0, \epsilon) \textrm{  for some  } \epsilon > 0 \iff f'(x;d) < 0
\end{align*}
\pagebreak

\sheader{Gradients} If $f: \mathbb{R}^n \to \mathbb{R}$ is continuously differentiable,
the gradient of $f$ at $x$ is the vector:
\begin{align*}
    \nabla f(x) = \begin{bmatrix}
        \frac{\partial f}{\partial x_1}(x) \\
        \vdots\\
        \frac{\partial f}{\partial x_n}
    \end{bmatrix}
    \in \mathbb{R}^n
\end{align*}
It is also implied that: $f'(x; d) = \nabla f(x)^T d$. And, if $\norm{d} = 1$, then the projection 
of the gradient onto $d$ is given by $f'(x;d) \cdot d$.
\gap
\sheader{Level Set (definition)} The $\alpha$-level set of $f$ is the set of points of $x$ where
the function value is at most $\alpha$. A direction $d$ points into the level set $[f \leq f(x)]$
if $f'(x;d) < 0$.
\gap
\header{Multidimensional Conditions}
\sheader{Matrix Definiteness} Let $A$ be an $n \times n$ matrix with $A = A^T$ (symmetric).
\begin{itemize}
    \item \textbf{Positive Semidefinite}: $A$ is positive semidefinite $(H \succeq 0)$ if:
    \begin{itemize}
        \item $x^T A x \geq 0 \,\, \forall \,\, x\in \mathbb{R}^n$
        \item For a diagonal matrix $D \succeq 0 \iff d_i \geq 0 \,\, \forall i$
        \item All eigenvalues are greater than or equal to zero.
        \item $A = R^TR$ for some $n \times n$ matrix $R$.
    \end{itemize}
    \item \textbf{Positive Definite} $(A \succ 0)$ if
    \begin{itemize}
        \item $x^T A x > 0 \,\, \forall \,\, 0 \neq x \in \mathbb{R}^n$
        \item For a diagonal matrix $D \succ 0 \iff d_i > 0 \,\, \forall i$
        \item All eigenvalues are strictly greater than zero.
        \item $A = R^TR$ for some nonsingular $n \times n$ matrix $R$.
    \end{itemize}
    \item \textbf{Indefinite} if:
    \begin{itemize}
        \item $\exists x \neq y \in \mathbb{R}^n \textrm{ such that } x^TA(x) > 0 \wedge y^TAy < 0$.
    \end{itemize}
\end{itemize}
\sheader{Hessians} For $f: \mathbb{R}^n \to \mathbb{R}$, twice continuously differentiable, the \textbf{Hessian}
of $f$ at $x \in \mathbb{R}^n$ is the $n\times n$ symmetric matrix:
\begin{align*}
    H(x) = \begin{bmatrix}
        \frac{\partial^2 f}{\partial x_1^2}(x) & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n}(x)\\
        \vdots & \ddots &\vdots \\
        \frac{\partial^2 f}{\partial x_n \partial x_1}(x) & \cdots & \frac{\partial^2 f}{\partial x_n^2}(x)
    \end{bmatrix}
\end{align*}

\sheader{Quadratic Functions} Quadratic functions take the following forms:
\begin{itemize}
    \item $f(x) = \frac{1}{2} x^T Hx + b^T x + \gamma$
    \item $\nabla f(x) = Hx + b$
    \item $\nabla^2 f(x) = H$
\end{itemize}

\sheader{Directional Second Derivatives} The directional second derivative of $f$ at $x$ in the 
direction $d$ is given by:
\begin{align*}
    f''(x;d) = \lim_{\alpha \to 0^+} \frac{f'(x + \alpha d; d) - f'(x; d)}{\alpha} = d^T \nabla^2 f(x)d
\end{align*}

\sheader{Linear and Quadratic Approximations}
\begin{itemize}
    \item Linear Approximation: $f(y) = f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f(z) (y-x)$
    \item Quadratic Approximation: $f(x + d) = f(x) + \nabla f(x)^T d + \frac{1}{2} d^T \nabla^2 f(x)d + o(\norm{d}^2)$
\end{itemize}
\pagebreak 

\sheader{Sufficient Conditions for Optimality} For $f: \mathbb{R}^n \to \mathbb{R}$ twice 
continuously differentiable and $\overline{x} \in \mathbb{R}^n$ stationery ($\nabla f(\overline{x}) = 0$), if:
\begin{itemize}
    \item $\nabla^2 f(\overline{x}) \succeq 0 \implies \overline{x}$ is a local min.
    \item $\nabla^2 f(\overline{x}) \preceq 0 \implies \overline{x}$ is a local max.
    \item $\nabla^2 f(\overline{x}) \succ 0 \implies \overline{x}$ is a \textit{strict} local min.
    \item $\nabla^2 f(\overline{x}) \prec 0 \implies \overline{x}$ is a \textit{strict} local max.
    \item $\nabla^2 f(\overline{x})$ indefinite, then $\overline{x}$ is undetermined. (test does not tell us anything)
\end{itemize}

\header{Nonlinear Least Squares}
We define the nonlinear least-square problem (NLLS) to be:
\begin{align*}
    \min_{x\in \mathbb{R}^n} f(x) := \frac{1}{2}\norm{r(x)}_2^2 && r: \mathbb{R}^n \to \mathbb{R}^m
\end{align*}
Where $r$ is the residual function is given by:
\begin{align*}
    r(x) = \begin{bmatrix}
        r_1(x)\\
        r_2(x)\\
        \vdots \\
        r_m(x)
    \end{bmatrix}, &&
    J(x) = \begin{bmatrix}
        \nabla r_1(x)^T\\
        \nabla r_2(x)^T\\
        \vdots\\
        \nabla r_m(x)^T
    \end{bmatrix},
    &&
    \nabla f(x) = J(x)^T r(x)
\end{align*}
This reduces to linear least-squares when $r$ is \underline{affine}.
\gap
\header{Descent Methods}
\sheader{Gradient Descent}
Intialization: Choose $x_0 \in \mathbb{R}^n$ and tolerance $\epsilon > 0$.\\
Iterations:
\begin{enumerate}
    \item Choose step size $\alpha^{(k)}$ to approximatley minimize $\ds \phi(\alpha) = f(x^k - \alpha \nabla f(x^k))$
    \item Update: $x^{(k + 1)} = x^{(k)} - \alpha^{(k)}\nabla f(x^{(k)})$
    \item Stop: if $\norm*{\nabla f(x^{(k)})} < \epsilon$.
\end{enumerate}

\sheader{Scaled Descent} Scaled descent is motivated by the zig-zagging behaviour of gradient 
descent, where exact linesearch implies that descent ``steps'' must be orthogonal to each other.
Specifically, if the condition number $\kappa$ is large, $\kappa \gg 1$, then the zig-zagging is 
exacerbated. Hence, scaled gradient seeks to make a change of variables $x = Sy$ for some 
nonsingular $S$. Hence, for an original minimization problem:
\begin{align*}
    \min_{x}f(x) && f: \mathbb{R}^n \to \mathbb{R}
\end{align*}
The change of variables implies:
\begin{align*}
    \min_y g(y) = f(Sy).
\end{align*}
The gradient of $g$ is given by:
\begin{align*}
    \nabla g(y) = S^T\nabla f(Sy).
\end{align*}
And thus, we get that the scaled gradient method is:
\begin{align*}
    x^{(k + 1)} = x^{(k)} + \alpha^{(k)}d^{(k)} && d^{(k)} = -SS^T \nabla f(x^{(k)}).
\end{align*}
Where $SS^T \succ 0$ (given earlier in Matrix Deefiniteness).
\gap
Hence, we can give a method for scaled gradient descent:
\begin{enumerate}
    \item Choose some scaling matrix $D^{(k)} = SS^T \succ 0$.
    \begin{itemize}
        \item Remark: choosing a scaling matrix $S$ is generally done in a way which makes 
        the condition number of the rescaled matrix as close to 1 as possible ($\kappa(\nabla^2 g) \approx 1$).
        \item Some common scalings include:
        \begin{align*}
            S^{(k)} (S^{(k)})^T = \begin{cases}
                (\nabla f(x^{(k)}))^{-1} & \textrm{Newton } (\kappa = 1)\\
                (\nabla f(x^{(k)}) + \lambda I)^{-1} & \textrm{Damped Newton}\\
                \textbf{Diag}(\frac{\partial f(x^{(k)})}{\partial x_i^2})^{-1} & \textrm{diagonal scaling}
            \end{cases}
        \end{align*}
    \end{itemize}
    \item Compute $d^{(k)} = -D^{(k)}\nabla f(x^{(k)})$. 
    \item Choose stepsize $\alpha^{(k)} > 0$ through linesearch.
    \item Update $x^{(k + 1)} = x^{(k)} + \alpha^{(k)}d^{(k)}$.
\end{enumerate}
\sheader{Gauss Newton for NLLS}\\
Objective: $\ds \min_{x} f(x) = \frac{1}{2} \norm{r(x)}_2^2$\\
Step: $\ds x^{(k + 1)} = x^{(k)} + \alpha^{(k)} d^{(k)}$\\
Procedure:
\begin{enumerate}
    \item Compute residual $r_k = r(x^{(k)})$ and Jacobian $J_k = J(x^{(k)})$
    \item Compute step $d^{(k)} = \textrm{argmin}_d \norm{J_k d + r_k}^2$, $(d^{(k)} = -J_k \backslash r_k)$.
    \item Choose stepsize $\alpha^{(k)} \in (0, 1]$ via linesearch on $f(x)$.
    \item Update $x^{(k + 1)} = x^{(k)} + \alpha^{(k)}d^{(k)}$.
    \item Stop if $\norm{r(x^{(k + 1)})} < \epsilon$ or $\norm{\nabla f(x^{(k)})} = \norm{J_k^T r_k} < \epsilon$.
\end{enumerate}
\sheader{Step Size Selection}
\begin{itemize}
    \item \textbf{Exact} (hard/expensive except for quadratic case): $\ds \alpha^{(k)} \in \textrm{argmin}_{\alpha \geq 0} \phi(\alpha)$.
    \begin{itemize}
        \item Quadratic Case: Choose $\ds \alpha^* = - \frac{\nabla f(x)^T d}{d^T Hd}$
    \end{itemize}
    \item \textbf{Constant} (cheap/easy, but requires analysis of $f$): $\ds \alpha^{(k)} = \overline{\alpha} > 0, \,\, \forall k$.
    \begin{itemize}
        \item Quadratic Case: Choose $\ds \alpha \in \left(0, \frac{2}{\lambda_{\textrm{max}}(H)}\right)$
    \end{itemize}
    \item \textbf{Approximate} [Backtracking/Armijo] (cheap, no analysis):
    \begin{enumerate}
        \item Set $\alpha^{(k)} > 0$
        \item Until $f(x^{(k)} + \alpha^{(k)}d^{(k)} < f(x^k) + \mu \alpha f'(x;d))$, $\mu \in (0, 1)$.
        \begin{itemize}
            \item $\alpha^{(k)} \leftarrow \alpha^k \cdot \rho$, $\rho \in (0, 1)$.
        \end{itemize}
        \item Return $\alpha^{(k)}$
    \end{enumerate}

\end{itemize}

\sheader{Lipschitz Smooth Functions}
A function $f: \mathbb{R}^n \to \mathbb{R}$ is said to be $L$-Lipschitz smooth if:
\begin{align*}
    \norm{\nabla f(x) - \nabla f(y)} \leq L\norm{x - y}\,\,\, \forall x, y \in \mathbb{R}^n
\end{align*}
For quadratic functions, we can say that $L = \lambda_{\max} (H)$.
\sgap
\sheader{Second-order L-smooth characterization} If $f$ is twice continuously differentiable,
then $f$ is $L$-Lipschitz smooth iff its Hessian is bounded by $L\,\, \forall x \in \mathbb{R}^n$:
\begin{align*}
    LI - \nabla^2 f(x) \succeq 0.
\end{align*}
\pagebreak

\header{Cholesky Factorization}

Cholesky factorization is another way of obtaining a LU-like decomposition, however, it only works
if the matrix is positive definite. In relation to the LU decomposition, it uses $(1/3)n^3$ flops
vs $(2/3)n^3$ for LU factorization. It is as follows:

\begin{align*}
    A = R^T R
\end{align*}

Where $R$ is some positive definite upper triangular matrix.
\gap
\header{Newton's Method}
Newton's method arises as a product from the second order approximation of $f$ at $x^{(k)}$. Hence,
we get two forms of Newton's method:
\gap
\sheader{Pure Newton's Method}
\begin{align*}
    x^{(k + 1)} = x^{(k)} + d_N^{(k)} && H_k d_N^{(k)} = - \nabla f(x^{(k)})
\end{align*}
\sheader{Damped Newton's Method}
\begin{align*}
    x^{(k + 1)} = x^{(k)} + \alpha d_N^{(k)} && \alpha \in (0, 1]
\end{align*}

For Newton's method to converge, we require that $\nabla^2 f(x^{(k)}) \succ 0 \,\, \forall k$ 
to ensure descent. However, it is important to note that this does not always hold, such 
as in the case that $\lambda_{\min} (H_k)$ is small.
\gap 
We can also use the Cholesky factorization for Newton's method. This process looks like:
\begin{enumerate}
    \item Choose $\tau = 0$
    \item Find the Cholesky factorization: $(H_k + \tau I) = R^TR$
    \begin{itemize}
        \item If the Cholesky fails, increase $\tau$ and repeat.
    \end{itemize}
    \item Solve $R^TR d_N^k = -g_k$.
\end{enumerate}

\header{Linear Constraints}

We can define a linearly constrained problem to be one such that:
\begin{align*}
    \min_{x \in \mathbb{R}^n} \curly{f(x) \,|\, Ax = b}
\end{align*}
Where:
\begin{itemize}
    \item $f: \mathbb{R}^n \to \mathbb{R}$ is a smooth function
    \item $A$ is $m \times n$, $m < n$
    \item $b \in \mathbb{R}^m$
    \item A has full rank.
\end{itemize}
\sheader{Feasible Set} A feasible set is the set of all vectors which satisfy the equation 
$Ax = b$:
\begin{align*}
    \mathcal{F} = \curly{x \in \mathbb{R}^n \,|\, Ax = b}.
\end{align*}
We can also represent the feasible set in an alternative fashion:
\begin{align*}
    \mathcal{F} = \curly{x \in \mathbb{R}^n \, | \, Ax = b} = \curly{\overline{x} + Zp \,| \, p \in \mathbb{R}^{n - m}}
\end{align*}
Where: \begin{itemize}
    \item $\overline{x}$ is a particular solution ($A\overline{x} = b$)
    \item $Z$ is a basis for the null space of $A$ ($AZ = 0$)
\end{itemize}
Hence the problem becomes unconstrained in $n - m$ variables:
\begin{align*}
    \min_{p \in \mathbb{R}^{n -m}} f(\overline{x} + Zp).
\end{align*}
We can then apply any optimization to obtain the solution $p^*$, and $x^*= \overline{x} + Zp^*$
is the solution to the original problem.

\pagebreak

\begin{center}
    \Large Second Half of the Course\\
    \normalsize
\end{center}

\header{Convex Sets}

\sheader{Definition} A set $C \subseteq \mathbb{R}^n$ is said to be \textit{convex} if for 
any $\mathbf{x}, \mathbf{y} \in C,\,\, \lambda \in [0, 1]$, the point 
$\lambda \mathbf{x} + (1-\lambda)\mathbf{y}\in C$.
\gap
\sheader{Convex Hull} The convex hull of a set of points $\mathcal{S}$
contains all convex combinations of points in $\mathcal{S}$:
\begin{align*}
    \textrm{conv}(\mathcal{S}) = \curly{\sum_{i=1}^k \theta_i x_i \,|\, x_i \in \mathcal{S}, \, \sum_{i = 1}^k \theta_i = 1, \theta_i \geq 0}
\end{align*}
Equivalently, we can say that $\mathcal{C} \subset \mathbb{R}^n$ is 
convex if it contains all convex combinations of its elements, i.e. 
$\mathcal{C} = \textrm{conv}(\mathcal{C})$.
\gap
\sheader{Subspace} $\mathcal{S} \subset \mathbb{R}^n$ is a subspace 
if it contains all linear combinations of points in the set, i.e.
\begin{align*}
    \alpha x + \beta y \in \mathcal{S}, \forall x, y \in \mathcal{S}, \forall \alpha, \beta \in \mathbb{R}
\end{align*}
For any $m \times n$ matrix $A$, its range and nullspace are 
subspaces of $\mathbb{R}^n$:
\begin{align*}
    \textbf{range}(A) = \curly{Ax \,|\, x \in \mathbb{R}^n} && \textrm{and} &&
    \textbf{null}(A^T) = \curly{z \,|\, A^Tz = 0}
\end{align*}

\sheader{Affine Sets} $\mathcal{L}$ is an affine set if it's a translated 
subspace, that is, for fixed $x_0 \in \mathbb{R}^n$ and subspace $\mathcal{S}$:
\begin{align*}
    \mathcal{L}= \curly{x_0 + v \,|\, v \in \mathcal{S}} \equiv x_0 + \mathcal{S}
\end{align*}

\sheader{Halfspaces and Hyperplanes} For some fixed nonzero vector
$a \in \mathbb{R}^n$ and scalar $\beta$, a hyperplane is given by:
\begin{align*}
    \mathcal{H} = \curly{x \,|\, a^Tx = \beta}
\end{align*}
and a halfspace is given by:
\begin{align*}
    \mathcal{H}_- = \curly{x \,|\, a^Tx \leq \beta}
\end{align*}
\begin{itemize}
    \item $a$ is normal to the hyperplane.
    \item Hyperplanes are both \textbf{affine} and \textbf{convex}.
    \item Halfspaces are convex but \textbf{not affine}.
\end{itemize}

\sheader{Cones} A set $\mathcal{K} \subset \mathbb{R}^n$ is a 
\textbf{cone} if $x \in \mathcal{K} \iff ax \in \mathcal{K},\,\, \forall \alpha \geq 0$.
\gap 
\sheader{Convex Cones} A convex cone is a cone that is 
also convex:
\begin{align*}
    x, y \in \mathcal{K} \wedge \alpha, \beta \geq 0 \implies \alpha x + \beta y \in \mathcal{K}
\end{align*}

Some examples of convex cones include:
\begin{itemize}
    \item Nonnegative Orthant: $\ds \mathbb{R}^n_+ = \curly{x \,|\, x_i \geq 0, i = 1, \ldots, n}$
    \item Second-Order Cone: $\ds \mathcal{L}_+^n = \curly{\begin{bmatrix}
        x\\t
    \end{bmatrix} \in \mathbb{R}^{n+1} \,\biggr\rvert\, \norm*{x}_2 \leq t, x \in \mathbb{R}^n, t \in \mathbb{R}}$
    \item Positive Semidefinite Cone
\end{itemize}

\sheader{Operations that Preserve Convexity} Let $\mathcal{C}_1, \mathcal{C}_2$ 
be convex sets in $\mathbb{R}^n$.
\begin{itemize}
    \item Nonnegative Scaling: $\theta \mathcal{C}_1 = \curly{\theta x \,|\, x \in C_1}, \,\, \theta \geq 0$.
    \item Intersection: $\mathcal{C}_1 \cap \mathcal{C}_2$.
    \item Sum: $\mathcal{C}_1 + \mathcal{C}_2 = \curly{x + y \,|\, x \in \mathcal{C}_1, \, y \in \mathcal{C}_2}$.
\end{itemize}

\sheader{Convex Polytopes} $\mathcal{S}$ is a \textbf{convex polytope} 
if it is the intersection of a fininte number of halfspaces:
\begin{align*}
    \mathcal{S} = \bigcap\limits_{i=1}^m \curly{x \,|\, a_i^T x \leq \beta_i} = \curly{x \,|\, Ax \leq b}
\end{align*}
where:
\begin{align*}
    A = \begin{bmatrix}
        a_1^T \\ \vdots \\ a_m^T 
    \end{bmatrix} \in \mathbb{R}^{m \times n} && \textrm{and}&& b \in \mathbb{R}^m
\end{align*}
\sheader{$n$-dimensional simplex} Is the intersection of $n$ halfspaces and a 
hyperplane:
\begin{align*}
    \mathcal{C} = \curly{x \,\biggr\rvert\, \sum_{i=1}^{n}= 1, \, x_i \geq 0}.
\end{align*}

\header{Convex Functions}
\sheader{Convexity} A function $f\,:\, C \to \mathbb{R}$ is \textbf{convex}
if $\mathcal{C} \subset \mathbb{R}^n$ is convex for all $x, y \in \mathcal{C}$
and $\theta \in [0, 1]$:
\begin{align*}
    f(\theta x + (1- \theta) y)\leq \theta f(x) + (1-\theta)f(y).
\end{align*}
\sheader{Strict Convexity} A function $f$ is strictly convex if the 
inequality is strict for $x \neq y$ and $\theta \in (0, 1)$:
\begin{align*}
    f(\theta x + (1-\theta)y) < \theta f(x) + (1- \theta)f(y).
\end{align*}
\sheader{Concavity} $f$ is concave if $(-f)$ is convex.
\gap
\sheader{Examples of Convex Functions}
\begin{itemize}
    \item Exponential: $e^{ax}$
    \item Powers: $x^\alpha$ over $x\geq $ for any $\alpha \geq 1$ or $\alpha \leq 0$
    \item Absolute Value: $|x|^\alpha$ for any $\alpha \geq 1$
    \item Norms: $\norm*{x}_p$ for any $p \geq 1$
\end{itemize}
\sheader{Examples of Concave Functions}
\begin{itemize}
    \item Powers: $x^\alpha$ over $x\leq 0$ for any $0 < \alpha < 1$
    \item Logarithm: $\log x$ over $x>0$
\end{itemize}
\sheader{Convex and Concave}
\begin{itemize}
    \item Affine: $a^Tx + \beta$ for any $a \in \mathbb{R}^n$ and $\beta \in\mathbb{R}$
\end{itemize}

\sheader{Restriction to Lines} $f : \mathbb{R}^n \to \mathbb{R}$ is 
\textbf{convex} if and only if 
\begin{align*}
    \phi(a) = f(x + \alpha d)
\end{align*}
is convex over $\alpha \in \mathbb{R}$ for all points $x$ and 
directions $d$.
\gap
\sheader{Operations that Preserve Convexity}
\begin{itemize}
    \item Nonnegative Scaling
    \item Sums
    \item Composition with Affine Function
\end{itemize}

\sheader{Convex Optimization}
\begin{align*}
    \min_{x \in \mathcal{C}} f(x), \,\,\, \mathcal{C} \subset \mathbb{R}^n \textrm{ convex}, \,\,\, f: \mathcal{C}\to \mathbb{R} \textrm{ convex}
\end{align*}
If $x^*$ is a \textbf{local minimizer}, it is also a \textbf{global minimizer}.
\pagebreak

\sheader{Convexity and Level Sets} Given the \underline{level set} of $f : \mathbb{R}^n \to \mathbb{R}$
at level $\alpha \in \mathbb{R}$:
\begin{align*}
    [f \leq \alpha] := \curly{x \in \mathbb{R}^n \,|\, f(x) \leq \alpha}
\end{align*}
If $f$ is convex, then all level sets are convex.
\gap
\sheader{First-Order Characterizations} Let $f : \mathcal{C} \to \mathbb{R}$
be differentiable over $\mathcal{C} \subset \mathbb{R}^n$. Then $f$ is 
convex if and only if:
\begin{align*}
    f(y) \geq f(x) + \nabla f(x)^T(y-x) \forall x, y \in \mathcal{C}.
\end{align*}

\sheader{Second-Order Characterization} Let $f : \mathcal{C} \to \mathbb{R}$
be twice differentiable over $\mathcal{C} \subset \mathbb{R}^n$. Then 
$f$ is convex if and only if:
\begin{align*}
    \nabla^2 f(x) \succeq 0\,\,\,\, \forall x \in \mathcal{C}.
\end{align*}

\header{Projected Gradient Descent}

For a set ${C} \subset \mathbb{R}^n$ closed convex, the 
\textbf{projection} of a point $x \in \mathbb{R}^n$ onto 
$C$ is the point:
\begin{align*}
    \mathbf{proj}_C(x) = \underset{z\in C}{\operatorname{argmin}}\norm*{x-z}.
\end{align*}
It is also important to note that:
\begin{align*}
    z = \mathbf{proj}_C(x) &\iff z\in C \wedge (x-z)^T(y-z)\leq 0\,\, \forall y\in C\\
    z = \mathbf{proj}_C(x) &\iff -\nabla g(z) = x- z \in \mathcal{N}_C(z)
\end{align*}
\sheader{Projection onto Affine Set} Given some set 
$C = \curly{z \in \mathbb{R}^n \,|\, Az = b}$ for $A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m$, 
then:
\begin{align*}
    \mathbf{proj}_C(x) = \textrm{argmin}\curly{\frac{1}{2} \norm*{z-x}^2 \,|\, Az = b}.
\end{align*}

Because $\mathcal{N}_C = \mathbf{range}(A^T)$, the optimality condition 
is such that:
\begin{align*}
    x - \mathbf{proj}_C(x) \in \mathbf{range}(A^T)
\end{align*}

\sheader{Projected Gradient Descent} Projected gradient 
descent proceeds as follows:
\begin{align*}
    \min_x \curly{f(x) \,|\, x \in C},
\end{align*}
for some $f : \mathbb{R}^n \to \mathbb{R}$ convex and smooth
and $C \subset \mathbb{R}^n$.

Hence, the algorithm for this is as follows:
\begin{itemize}
    \item Start from $x_0 \in C$
    \item For $k = 0, 1, 2, \ldots$
    \begin{itemize}
        \item $g_k = \nabla f(x_k)$
        \item Linesearch on $\phi(\alpha) = f(\mathbf{proj}_C(x_k - \alpha g_k))$
        \item $x_{k+1} = \mathbf{proj}_C(x_k - \alpha_k g_k)$
        \item Stop if $||x_{k+1} - x_k||$ is small.
    \end{itemize}
\end{itemize}

\sheader{Stationarity}
\begin{align*}
    x^* \oper{argmin}{x\in C} f(x) \iff x^* = \mathbf{proj}_C(x^* - \alpha \nabla f(x^*))\,\,\, \forall \alpha >0
\end{align*}
By the projection theorem:
\begin{align*}
    (x^* - \alpha \nabla f(x^*) - x^*)^T (z-x^*)\leq 0 \,\, \forall z \in C\\
    -\alpha \nabla f(x^*)^T(z-x^*) \leq 0\,\,\, \forall z \in C
\end{align*}
Given the definition of a normal cone, we obtain that:
\begin{align*}
    -\nabla f(x^*) \in \mathcal{N}_C(x^*).
\end{align*}

\pagebreak

\header{Convex Optimality}

Given some convex optimization problem:
\begin{align*}
    \min_x \curly{f(x) \,|\, x \in C},
\end{align*}
we say that:
\begin{itemize}
    \item if $C = \mathbb{R}^n$ the problem is \textbf{unconstrained}, so $\nabla f(x^*) = 0$.
    \item however, if $C \neq \mathbb{R}^n$, then this does not imply that $\nabla f(x^*) = 0$.
\end{itemize}

\sheader{Normal Cone} The normal cone to the set $C \subset \mathbb{R}^n$
at the point $x \in C$ is the set:
\begin{align*}
    \mathcal{N}_C(x) = \curly{d \in \mathbb{R}^n \,|\, d^T(z-x) \leq 0, \,\, \forall z \in C}
\end{align*}

\sheader{Necessary and Sufficient Optimality}
A point $x^* \in \textrm{argmin}_{x\in C} f(x)$ if and only if:
\begin{align*}
    \nabla f(x^*)^T (x- x^*)\geq 0 \forall x \in C.
\end{align*}
Using the definition of the normal cone we can deduce the equivalent condition:
\begin{align*}
    -\nabla f(x^*) \in \mathcal{N}_C(x^*).
\end{align*}

\sheader{Interior Point} It is implied that points in the interior 
have empty sets as normal cones.

This implies for unconstrained optimality that $\nabla f(x^*) = 0$.
\gap
\sheader{Normal Cone to an Affine Set} Given some 
\begin{align*}
    C = \curly{x \in \mathbb{R}^n \,|\, Ax = b}.
\end{align*}
Then for any $x \in C$, define the translated set as:
\begin{align*}
    C_x = \curly{z - x \,|\, z \in C}.
\end{align*}
Then,
\begin{align*}
    \mathcal{N}_C(x) = \mathbf{range}(A^T) = A^Ty
\end{align*}
So $x^*$ is optimal iff $Ax^* = b$ and $-\nabla f(x^*) = A^Ty$ for some 
$y \in \mathbb{R}^n$.
\gap
And such, the vector $y \in \mathbb{R}^m$ contains all of the 
\textbf{Lagrange Multipliers} for each constraint: $a_i^Tx = b_i$.
\gap
\header{Convergence of Gradient Descent}
\begin{itemize}
    \item Quadratic functions with $f(x) = \frac{1}{2}x^TAx + b^Tx + \gamma$,
    with $A \succeq 0$, has $L = \norm*{A}_2 = \lambda_{\textrm{max}}(A)$
    \item If $f$ twice continuously differentiable, then $f$ is $L$-smooth
    if and only if, for all $x$:
    \begin{align*}
        \nabla^2f(x)\preceq LI \iff \norm*{\nabla^2 f(x)}_2 \leq L.
    \end{align*}
\end{itemize}

\sheader{Descent Lemma}
If $f$ is $L$-smooth, then for all $x, z$, then $f$ is \textbf{globally majorized}
by a quadratic approximation.
\begin{align*}
    f(z) \leq f(x) + \nabla f(x)^T (z-x) + \frac{L}{2} \norm*{z-x}^2
\end{align*}

From prior it is known that we get decreasing objecitive values if we 
choose some stepsize $\alpha \in \left(0, \frac{2}{L}\right)$.

\pagebreak

\header{Strong Convexity}
A function $f : \mathbb{R}^n \to \mathbb{R}$ is $\mu$-strongly convex (with $\mu > 0$)
if for all $x, y$:
\begin{align*}
    f(z) \geq f(x) + \nabla f(x)^T(z-x) + \frac{\mu}{2}\norm*{z-x}^2
\end{align*}

If $f$ is \textbf{twice continuously differentiable}, then $f$ 
is $\mu$-strongly convex if and only if for all $x$:
\begin{align*}
    d^T\nabla^2 f(x) d \geq \mu \norm*{d}^2\,\,\, \forall d \in \mathbb{R}^n \iff \nabla^2 f(x) \succeq I\mu
\end{align*}

Alternatively, one can say that a function $f$ is $\mu$-strongly convex 
if and only if for all $x$,
\begin{align*}
    g(x) = f(x) - \frac{\mu}{2}\norm*{x}^2
\end{align*}
This also implies that Tikhonov regularization induces strong convexity.

\sheader{Distance to Solution} Some important lemmas arise out of this.
\begin{enumerate}
    \item If $f$ is $L$-smooth, then for all $x$ and all minimizers $x^*$
    with $f^* = f(x^*)$:
    \begin{align*}
        \frac{1}{2L} \norm*{\nabla f(x)}^2 \leq f^* \leq \frac{L}{2}\norm*{x-x^*}^2
    \end{align*}
    \item If $f$ is $\mu$-strongly convex, then for all $x$ and all 
    minimizers $x^*$ with $f^* = f(x^*)$:
    \begin{align*}
        \frac{\mu}{2}\norm*{x-x^*}^2 \leq f(x) - f^* \leq \frac{1}{2\mu} \norm*{\nabla f(x)}^2
    \end{align*}
\end{enumerate}

The sum of all of these conclusions implies that the Hessian eigenvalues 
can be bounded from above and below:
\begin{align*}
    \mu I \preceq \nabla^2 f(x) \preceq LI
\end{align*}

We can also deduce the number of iterations $T$ it would take 
to get down to a desireable error:
\begin{align*}
    T \leq \frac{L}{\mu}\log\left(\frac{f_0 - f^*}{\epsilon}\right)
\end{align*}

\header{Stochastic Gradient Descent}

For some least-squares gradient descent problem, where one would have that:
\begin{align*}
    \nabla f(x) = \sum_{i=1}^{N}\nabla f_i(x)
\end{align*}

However, we could take samples $j = 1, \ldots, N$ that occur with 
equal probability $\frac{1}{N}$ to get that:
\begin{align*}
    f(x) = \frac{1}{N}\sum_{j=1}^{N}f_j(x)= \mathbb{E}_j f_j(x)
\end{align*}
Hence, tha gradient is that:
\begin{align*}
    \nabla f(x) = \frac{1}{N}\sum_{j=1}^{N}\nabla f_j(x) = \mathbb{E}_j \nabla f_j(x)
\end{align*}

Hence, we randomly sample a small patch of observations $\mathcal{B} \subseteq \curly{1, \ldots, N}$,
then:
\begin{align*}
    g_\mathcal{B}(x) = \frac{1}{|\mathcal{B}|}\sum_{i\in \mathbb{B}}\nabla f_i(x) \implies \mathbb{E}_\mathcal{B}g_\mathcal{B}(x) = \nabla f(x)
\end{align*}

This is called a ``stochastic approximation''.

\pagebreak

\sheader{Stochastic Gradient Descent}

\begin{align*}
    \min_{x\in \mathbb{R}^n} f(x) = \frac{1}{N}\sum_{i=1}^{N}f_i(x)
\end{align*}

So each step we have that:
\begin{align*}
    x_{k+1} = x_k - \alpha_k \nabla g_k
\end{align*}

Where:
\begin{align*}
    g_k := \frac{1}{B_k}\sum_{i \in B_k} \nabla f_i(x_k)
\end{align*}
Where $B_k$ is a batch of uniformly random iid samples from $\curly{1, \ldots, N}$
\begin{itemize}
    \item Step length $\alpha_k$ often called ``Learning Rate'' in this context.
    \item Need to assume mean-squared error in stochastic approximation is bounded:
    \begin{align*}
        \mathbb{E}\left[\norm*{g_k - \nabla f(x_k)}^2\right] =
        \mathbb{E}\left[\norm*{g_k}^2\right] - \norm*{\nabla f(x)}^2 \leq \sigma^2
    \end{align*}
\end{itemize}

\sheader{Convergence in Expectation (constant steplength)} It works 
out such that:
\begin{align*}
    \mathbb{E}f_{k+1} \leq \mathbb{E} f_k - \frac{\alpha}{2} \mathbb{E} \norm*{\nabla f_k}^2 + \frac{\alpha^2 \sigma^2 L}{2}
\end{align*}
If we sum over $k = 0, 1, 2, \ldots, T$ and recurse, rearranging, we get that:
\begin{align*}
    \frac{1}{T}\sum_{k=1}^{T}\mathbb{E}\norm*{\nabla f(x_k)}^2 \leq \frac{2(f(x_0) - f^*)}{\alpha T} + \frac{\alpha \sigma L}{2}
\end{align*}

\header{Linear Programming}
\sheader{Conventional Program} Given $A \in \mathbb{R}^{m\times n}, b\in \mathbb{R}^m, c \in \mathbb{R}^n$:
\begin{align*}
    \oper{minimize}{x}\,\,\, &c^Tx\\
    \textrm{subject to } &Ax=b\\
    &x \geq 0
\end{align*}

\sheader{Diet Problem} 
\begin{itemize}
    \item Minimum-cost diet
    \item $x_i$ represents how many servings of food group $i$ to eat
    \item $c_i$ gives cost of 1 serving of food from group $i$
    \item $a_i^Tx = b_i$ encodes nutritional recommendations
    \item $x\geq 0$ since you can't eat negative food
\end{itemize}

\sheader{Geometry}

Consider a linear program to be in a polyhedron:
\begin{align*}
    \mathcal{P} = \curly{x \,|\, Ax \leq b}
\end{align*}

Then, an extreme point $x\in \mathcal{P}$ is if there \underline{does not exist}
two vectors $y, z \in \mathcal{P}$ such that:
\begin{align*}
    x = \lambda y  + (1-\lambda)z \textrm{  for any  } \lambda \in (0, 1).
\end{align*}

\sheader{Verticies} $x\in \mathcal{P}$ is a vertex of $\mathcal{P}$ 
if there exists a vector $c \neq 0$ such that:
\begin{align*}
    c^Tx < c^Ty \textrm{  for all  } y \in \mathcal{P}, y\neq x
\end{align*}
\begin{itemize}
    \item Given a vertex $x$, find $c$ such that $c^Tx < c^Ty$ for all $y \in \mathcal{P}$, $y\neq x$.
    \item Given a vector $c$, find $x$ such that $c^Tx < c^Ty$ for all $y \in \mathcal{P}, y\neq x$:
    \begin{align*}
        \oper{minimize}{x} \,\,\,c^Tx \textrm{  subject to  } x \in \mathcal{P}.
    \end{align*}
\end{itemize}

\sheader{Active Constraints} Define $\mathcal{B}$ as the set of 
\textbf{active} or \textbf{binding} constraints (at $x^*$):
\begin{itemize}
    \item Active Constraints: $a_i^Tx^* = b_i, i \in \mathcal{B}$
    \item Inactive Feasible Constraints: $a_i^Tx^* < b_i, i \in \mathcal{N}$
    \item Inactive Infeasible Constraints $a_i^Tx^* > b_i, i \notin \mathcal{B} \cup \mathcal{N}$
\end{itemize}

Hence, the subset of active constraints are as follows:
\begin{align*}
    A_\mathcal{B} = \overline{A} = \begin{bmatrix}
        a_{i_1}^T\\
        \vdots\\
        a_{i_k}^T
    \end{bmatrix} &&
    b_\mathcal{B} = \overline{b} = \begin{bmatrix}
        b_{i_1}\\
        \vdots\\
        b_{i_k}
    \end{bmatrix} && 
    \mathcal{B} = \curly{i_1, \ldots, i_k}
\end{align*}

\sheader{Basic Solutions} $x^*$ is a \textbf{basic solution} if one 
of the following equivalent conditions hold:
\begin{itemize}
    \item $a_{i_1}, a_{i_2}, \ldots, a_{i_n}$ are linearly independent
    \item $\overline{A}x^* = \overline{b}$ has a unique solution
    \item $\textrm{rank}(\overline{A}) = n$
\end{itemize}
\sheader{Basic Feasible Solution} $x^*$ is a basic solution and $x^* \in \mathcal{P}$.
\gap
The following are equivalent:
\begin{itemize}
    \item $x^*$ is a vertex
    \item $x^*$ is an extreme point 
    \item $x^*$ is a basic feasible solution
\end{itemize}

\sheader{Unbounded Directions} $\mathcal{P}$ contains a \textbf{half-line}
if there exists $d \neq 0$, $x_0$ such that:
\begin{align*}
    x_0 + \alpha d \in \mathcal{P} \textrm{  for all  } \alpha \geq 0
\end{align*}
\begin{itemize}
    \item $\mathcal{P}$ unbounded $\iff \mathcal{P}$ contains a half-line
    \item $\mathcal{P}$ has no extreme points $\iff \mathcal{P}$ contains a line
    \item $p^* = -\infty$ if and only if there exists a feasible half line
    \item $p^* = + \infty$ if and only if $\mathcal{P} = \emptyset$
    \item $p^*$ is finite if and only if $X^* \neq \emptyset$
\end{itemize}

\header{Simplex Method}

Assume standard form of an LP problem:
\begin{align*}
    \oper{minimize}{x} \,\,\, &c^Tx\\
    \textrm{subject to}\,\,\, &Ax=b, x\geq 0
\end{align*}
Assuming that:
\begin{itemize}
    \item $A$ has full row rank (no redundant rows)
    \item The LP is feasible
    \item All basic feasible solutions are nondegenerate
\end{itemize}
\pagebreak

We also define two variable index sets:
\begin{itemize}
    \item $\mathcal{B} = \curly{\beta_1, \beta_2, \ldots, \beta_m}$
    \item $\mathcal{N} = \curly{\eta_1, \eta_2, \ldots, \eta_{n-m}}$
\end{itemize}

\sheader{Feasible Directions} A direction $d$ is \textbf{feasible} at 
$x \in \mathcal{P}$ if there exists $\alpha > 0$ such that:
\begin{align*}
    x + \alpha d \in \mathcal{P}
\end{align*}

\sheader{Constructing Feasible Directions} Given some $x \in \mathcal{P}$
and $Ax = b$, $x\geq 0$, require that for all $\alpha \geq 0$ that:
\begin{align*}
    b = A(x + \alpha d) = Ax + \alpha A d = b + \alpha Ad
\end{align*}

Thus, we require that $Ad = 0$. Suppose that $x$ is a basic feasible solution,
such that:
\begin{align*}
    0 = Ad = \begin{bmatrix}
        B & N
    \end{bmatrix} \begin{bmatrix}
        d_b \\ d_n
    \end{bmatrix}
    = Bd_B + Nd_N \implies Bd_B = -Nd_N
\end{align*}

We then construct search directions by moving a \textbf{single} nonbasic
variable $\eta_k \in \mathcal{N}$:
\begin{align*}
    d_N = e_k \textrm{  and  } Bd_B = -Ne_k = -a_{\eta_k}
\end{align*}
This goes on with swapping variables inside and outside of the basic set.
\gap
\sheader{Choosing Swaps}
\begin{itemize}
    \item For choosing $d_N = e_p$, choose $p$ such that $z_{n_p} < 0$ (most negative) (for some $z = c - A^Ty$, $B^Ty = c_B$)
    \item Some basic variable $\beta_q$ must exit the basis, choose: $\ds q = \oper{argmin}{q|d_{\beta_q} < 0} \,\,\left( -\frac{x_{\beta_q}}{d_{\beta_q}}\right)$
\end{itemize}
Hence, the new basic and nonbasic variables are as follows:
\begin{itemize}
    \item $\mathcal{B} \leftarrow \mathcal{B} \setminus \curly{\beta_q} \cup \curly{\eta_p}$
    \item $\mathcal{N} \leftarrow \mathcal{N} \setminus \curly{\eta_p} \cup \curly{\beta_q}$
\end{itemize}

\header{Converting Linear Programs to Standard Form}

A generic polyhedron has the form:
\begin{align*}
    \mathcal{P} = \curly{x \,\Biggr\lvert\, \begin{matrix}
        Ax = b \\ Cx \leq d
    \end{matrix}}
\end{align*}
However, it follows that a standard-form polyhedron has the form:
\begin{align*}
    \mathcal{P} = \curly{x \,\Biggr\lvert\, \begin{matrix}
        Ax = b\\ x\geq 0
    \end{matrix}} \textrm{  with  } b \geq 0.
\end{align*}
\sheader{Converting to Standard Form}
\begin{enumerate}
    \item Positive $b$ and positive $d$:
    \begin{enumerate}
        \item For $b_i < 0$, replace:
        \begin{align*}
            a_i x = b_i \to (-a_i)x = (-b_i)
        \end{align*}
        \item For $d_i < 0$, replace:
        \begin{align*}
            c_i^Tx \leq d_i \to (-c_i)^T x \geq (-d_i)\\
            c_i^Tx \geq d_i \to (-c_i)^Tx \leq (-d_i)
        \end{align*}
    \end{enumerate}
    \item Free Variables 
    \begin{itemize}
        \item $x_i$ is called a \textbf{free variable} if it has 
        no constraints
        \item There are no free variables in standard form -- every 
        variable must be nonnegative
    \end{itemize}
    Converting Free Variables: Every free variable $x_i$ is replaced
    with two new variables $x_i'$ and $x_i''$:
    \begin{align*}
        x_i = x_i' - x_i'', \,\, x_i' \geq 0 \textrm{  and  } x_i'' \geq 0
    \end{align*}
    \begin{itemize}
        \item $x_i'$ encodes the positive part of $x_i$
        \item $x_i''$ encodes the negative part of $x_i$
        \item optimal solution must have that $x_i' \cdot x_i'' = 0$
    \end{itemize}
    \item Slack and Surplus
    \gap 
    For every inequality constraint of the form:
    \begin{align*}
        c_i^Tx \leq d_i && (c_i^Tx \geq d_i)
    \end{align*}
    Introduce a new \textbf{slack} (or \textbf{surplus}) variable $s_i$, 
    replacing the inequality with two constraints:
    \begin{align*}
        \begin{cases}
            c_i^Tx + s_i = d_i\\
            s_i \geq 0
        \end{cases}
        &&
        \left(
            \begin{cases}
                c_i^Tx - s_i = d_i\\
                s_i \geq 0
            \end{cases}
        \right)
    \end{align*}
\end{enumerate}
In standard form, there are:
\begin{itemize}
    \item $n$ variables $(x_1, \ldots, x_n)$
    \item $m + n$ total constraints
    \begin{itemize}
        \item $m$ equality constraints ($Ax = b$)
        \item $n$ inequality constraints ($x\geq 0$)
    \end{itemize}
\end{itemize}
For any basic solution $x$:
\begin{itemize}
    \item The basic set $\mathcal{B}$ must have $n$ elements
    \item Thus, exactly $n$ of the constraints need to be active at $x$
    \item $m$ equality constraints are always satisfied
    \item $n-m$ of the inequality constraints $x\geq 0$ should be ``active''
\end{itemize}

\header{Duality}
\sheader{Primal Problem}
\begin{align*}
    \oper{minimize}{x}\,\,\, c^Tx \textrm{  subject to  } Ax = b, \, x\geq 0
\end{align*}
\begin{itemize}
    \item $n$ variables, $m$ constraints
    \item optimal solution $x^*$
    \item optimal value $p^* \equiv c^T x^*$
\end{itemize}

\sheader{Relaxed Problem}
\begin{align*}
    \oper{minimize}{x}\,\,\, c^Tx + y^T(b-Ax) \textrm{  subj to  } x\geq 0
\end{align*}
\begin{itemize}
    \item the relaxed problem provides a lower bound for $p^*$:
    \begin{align*}
        g(y) := \min_{x\geq 0}\curly{c^T x + y^T(b-Ax)} \leq c^T x^* + y^T(b-Ax^*) = c^Tx^* = p^*
    \end{align*}
\end{itemize}
\sheader{Tightest Lower Bound} Find $y$ that solves
\begin{align*}
    \oper{maximize}{y}\,\,\, g(y).
\end{align*}
It follows that:
\begin{align*}
    g(y) = \begin{cases}
        b^Ty & \textrm{if } c - A^Ty \geq 0\\
        -\infty & \textrm{otherwise}
    \end{cases}
\end{align*}

Because we want to \textbf{maximize} $g(y)$, we must have that:
\begin{align*}
    \oper{maximize}{y} \,\,\, b^Ty && \iff && \oper{maximize}{y, z}\,\,\, b^Ty\\
    \textrm{subject to}\,\,\, c-A^Ty \geq 0 && && \textrm{subject to} \,\,\, A^Ty + z = c, z\geq 0
\end{align*}
This is the \textbf{dual} LP.
\gap
\sheader{Weak Duality} Suppose that $x$ is primal feasible:
\begin{align*}
    Ax = b,\,\, x\geq 0
\end{align*}
Suppose that $(y,z)$ is dual feasible:
\begin{align*}
    A^Ty + z = c, \,\, z \geq 0
\end{align*}
Then the primal objective is bounded below by the dual objective.
\begin{align*}
    c^Tx \geq y^T b.
\end{align*}
\sheader{Weak Duality Theorem} If $(x, y, z)$ is primal/dual feasible,
then:
\begin{itemize}
    \item The primal value is an upper bound for the dual value
    \item The dual value is a lower bound for the primal value
\end{itemize}

\sheader{Complementarity}
\begin{itemize}
    \item If $x^Tz= 0$ we say the bound is ``tight''.
\end{itemize}

\sheader{Strong Duality Theorem} If an LP has an optimal solution, so 
does its dual, and the optimal values are equal, ie, $p^* = d^*$.
\gap
\sheader{Theorem} The primal-dual triple $(x, y, z)$ is optimal if and 
only if:
\begin{enumerate}[(1)]
    \item $Ax = b, x\geq 0$
    \item $A^Ty + z = c, z\geq 0$
    \item $x^Tz = 0$
\end{enumerate}
\header{Matrix Games}
\begin{center}
    \begin{tabular}{|l|l|}
        \hline
        Player X & Player Y \\
        \hline
        $\oper{maximize}{x, \lambda}\,\,\, \lambda$ & $\oper{minimize}{y, \nu} \,\,\, \nu $\\
        $\textrm{subject to} \,\,\, \lambda e \leq Ax$ & $\textrm{subject to} \,\,\, \nu e \geq A^Ty$\\
        \hspace{1.7cm}$e^Tx$, $x\geq 0$ &\hspace{1.7cm}$e^Ty$, $y\geq 0$\\
        \hline
    \end{tabular}
\end{center}
\pagebreak

\header{Multiplicative Weights Update Method}
Given some game, where on round $t$:
\begin{itemize}
    \item Player picks weights $p_t$ in the \textbf{simplex} 
    \begin{align*}
        p_t \in \Delta_n = \curly{p \in [0, 1]^n : \sum_{i=1}^n p_i =1}
    \end{align*}
    \item Adversary picks losses $\ell_t \in [-1, 1]^n$ for the experts
    \item By the end of the round, player loses $p_t(i)\cdot \ell_t(i)$ for 
    stock/expert $i$:
    \begin{align*}
        \textrm{Loss on round }t = \sum_{i=1}^{n}\ell_t (i) \cdot p_t(i) = l_t^T p_t
    \end{align*}
\end{itemize}

We can evaluate the player through regret:
\begin{align*}
    \textrm{Regret}(T) = \sum_{t=1}^{T}\ell_t^T p_t - \min_{i\in [n]}\sum_{t=1}^{T}\ell_t(i)
\end{align*}
\textit{Note: Average regret goes to 0.}

\end{document}